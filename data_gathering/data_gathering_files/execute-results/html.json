{
  "hash": "e3a2a3d3e5f2d7a5cc184f74e804b843",
  "result": {
    "markdown": "---\ntitle: Data Gathering\nauthor: Powell Sheagren\n---\n\n\n# Data Gathering\n\n\n::: {.cell}\n\n:::\n\n\n### Introduction\n\nFor this project I will be accessing the Ravelry API for pattern data from the Database. When trying to find larger databases of patterns Ravelry is the only source which is open to the public and not proprietary to an organization. I was still concerned that I was overstepping my bounds in downloading the data that I actually sent an email to the api's email:\n\n![\"Me emailing the API dev and them saying it was cool\"](../images/Approval.png)\n\nWith this remarkable stamp of approval I began the process of pinging the API for pattern data. This also acted as my introduction to using API's formally and I was lucky to find an article which was an example of how to use API's with the Ravelry API. The article, linked below, was written expressly to help understand how API's worked and I used some of her code in order to set up my first API trials.\n\nhttps://medium.com/data-science-at-microsoft/how-to-access-an-api-for-first-time-api-users-879002f5f58d\n\n## Failed attempt\n\nAfter giving some general details about how API's work she provided example code which was as follows\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## I'm working on it\nimport http.client\nimport json\nimport requests\nimport pandas as pd\n\n## borrowed code\nauthUsername = RAVELRY_USERNAME\nauthPassword = RAVELRY_PASSWORD\n\n#define URL for the API request\nurl = 'https://api.ravelry.com/color_families.json'      \n#make the request\nr1 = requests.get(url, auth=requests.auth.HTTPBasicAuth(authUsername, authPassword))\n#close the connection\nr1.close()\n\nprint(\"response output formatted:\")\nprint(json.dumps(json.loads(r1.text), indent=4)) #makes the json more readable\n```\n:::\n\n\nI had already explored the Ravelry's API and documentation to understand what I was looking for. I had also made a pro account and a read key that I could use to get information and not much else, which was okay since I was just downloading information.\n\nI may put this in a citation/ https://www.ravelry.com/api#index\n\nHer example gave a strong sense of how the process operated with creating an account and accessing with get requests. The resulting JSON files that I got on a few attempts mostly had upper level dating missing the pattern information I was looking for.\n\nShe expanded upon the API's methods with classes and functions however she also linked to some other resources which included a package in R specifically designed for use with the Raverly API. So like most programmers I decided to use another's work for my own benefit.\n\nhttps://www.rdocumentation.org/packages/ravelRy/versions/0.1.0\n\n## Succesful attempt\n\n# README\nI am creating this table for future reference, Not all aspect here may be addressed in this initial draft. I also will not be evaluating a couple of the tables as the time to render would be insane. I'm working to convert it to ipynb or finding other workarounds. THis is a work in progress\n\nTable of process\n- Package overview\n- Description of process\n- Query step\n  - Query timing\n  - Query optimization\n    - Query timing\n    - Optimizing by page number\n  - Data collection through Query\n- Get pattern step\n  - Pattern timing\n  - Pattern optimization\n  - Pattern collection through iterative process\n- Summary of remaining data\n- conclusions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Never do work someone has done before\nlibrary(ravelRy)\n\"ravelRy is an R package that provides access to the Ravelry API.\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ravelRy is an R package that provides access to the Ravelry API.\"\n```\n:::\n\n```{.r .cell-code}\n\"Ravelry describes itself as a place for knitters, crocheters, designers, spinners, weavers and dyers to keep track of their yarn, tools, project and pattern information, and look to others for ideas and inspiration.\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Ravelry describes itself as a place for knitters, crocheters, designers, spinners, weavers and dyers to keep track of their yarn, tools, project and pattern information, and look to others for ideas and inspiration.\"\n```\n:::\n:::\n\n\n#### Package Overview\n\nThe package required the same username and password I had made before so I established them in my .Renviron folder so they could be accessed without console intervention. This step was required as the package would use the console to ask for username and password which stopped Quarto rendering.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Establishing Credentials\nravelry_auth(key = 'username') \nravelry_auth(key = 'password')\n```\n:::\n\n\nHaving set up, I was left to run my query and collect as many data entries as I could reasonably. The database itself has on my last check one million two hundred and sixty three thousand nine hundred and eleven patterns. While I do not need all the patterns I'm looking to get as many as reasonable which would be at maximum one million and minimum one hundred thousand.\n\n#### Description of Process\n\nOn my end this collection included two steps, the first of which involved a direct query of the database as if I were doing a search (search_patterns). I could have provided some specifics such as category or type but I was more concerned with having as much information at possible at this stage.\n\nThe second step involved using a built in function of the package to actually get the pattern information which I will be analyzing. Interestingly this step took more time to process than the previous and I did some experimenting to optimize the steps. I will mention that and more after I complete the demo of the data gathering process.\n\n#### Query Step\nAnd thus I began the process of querying data optimally and maximally\n###### Query Timing\nI am starting but doing some tests on query time in comparison to amount and so I took the length step of doing a for loop for each 10 queries. There is currently an abridged amount but the pattern should hold and I will prove that once I can process better.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_times <- c()\nstart_time <- Sys.time()\nfor(i in 1:750){\n  n_query <- 10*i\n  query_start_time <- Sys.time()\n  search_results <- search_patterns(query = '',page_size = n_query)\n  query_results_time <- Sys.time()\n  query_times <- c(query_times,query_results_time - query_start_time)\n}\nend_time <- Sys.time()\n\nquery_data <- data.frame((1:len(query_times))*10,query_times)\ncolnames(query_data)[1] <- \"query_amount\"\n\nmodel <- lm(data = query_data, query_times ~ query_amount)\nsummary(model)\n\na <- model$coefficients[2] # slope\nb <- model$coefficients[1] # intercept\n\nggplot(data = query_data, aes(x = query_amount,y=query_times)) + geom_point()\n```\n:::\n\n\nUpon visualization there is a fairly linear trend with some variance change as the query rate goes larger. This variance can probably be found in the difference in computational power in my computer moment to moment. However the linear trend, as shown by the quick regression model I computed, is a good sign that there aren't any artificial constraints on the system to limit querying. Also given the equation found of time = .0068(amount) + .174 we can guess that querying one million patterns will take about 6800 seconds or 113 minutes which is about 2 hours. It would be optimal to just plug that amount into the search patterns feature but there seems to be an unofficial limit to the amount of pattern that can be referenced which I will talk about next.\n\n###### Optimizing by page number\n\nAfter completing a large number of queries, most often at or above 5000, there will be an error that says a json wasn't read. This implies that there is a hard limit to how many patterns can be searched in a single query. The logical solution is to do multiple queries with a set amount. In the search_patterns function there is another variable aside from page_size, which gives the number of results to retrieve, page gives which page to start one. Now the page size on the website averages to 48, however I wanted to check where the function breaks pages. I'll be searching for 200 patterns starting at page 1 and then page 2 and then I will see what the index of the first value is on the second page.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(i in 1:10000){\n  n = 100\n  page_1 <- search_patterns(query = '',page_size = n,page =i)\n}\n\nn = 4000\npage_1 <- search_patterns(query = '',page_size = n,page = 1)\npage_2 <- search_patterns(query = '',page_size = n,page = 2)\npage_2_start <- page_2$name[1]\npage_1_check <- page_1 %>% filter(name == page_2_start)\n\npage_2_start\npage_1_check\npage_1\n```\n:::\n\n\n#### Data collection through Query\n\nAfter condensing a way to give a max query by page I can estimate a total amount of data of ____ which I will now query. Based on the equation given this is expected to take around ___ amount of time give or take a few minutes due to the added time of merging data sets between steps\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery_start_time <- Sys.time()\n\n# querying for majority of patterns in the database\nsearch_results <- search_patterns(query = '',page_size = 1000,page = 1)\nquery_results_time <- Sys.time()\n#\n# expanding data to get full pattern \n#patterns <- get_patterns(ids = intermediate$id)\n#query_patterns_time <- Sys.time()\n\nprint(query_results_time - query_start_time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 6.123998 secs\n```\n:::\n\n```{.r .cell-code}\n# Time keeper for Query\n# 10 queries in .5 second\n# 100 queries in 1 seconds\n# 1000 queries in 7.85 seconds\n# 5000 queries in 31.23 seconds\n# 7500 queries in \n```\n:::\n\n\nAs a part of the process I also completed the timely step of calculating the query time for each query. I may have over crashed my queries per day but I was having fun.\n\n## Get Pattern Step\n\n#### Pattern timing\n\nSimilar to the query time the pattern time tends to change with the amount of data so I will do a test run of timing for a possible optimization problem. I will again go by every 10 queries and will ignore the time taken to request the query. As this is a separate step in my real process I will not need to worry about batching it in the same amount as the queries\n\n\n::: {.cell}\n\n```{.r .cell-code}\npattern_times <- c()\n\nfor(i in 1:750){\n  n_query <- 10*i\n  search_results <- search_patterns(query = '',page_size = n_query)\n  pattern_start_time <- Sys.time()\n  get_patterns(id = search_results$id)\n  pattern_results_time <- Sys.time()\n  pattern_times <- c(pattern_times,pattern_results_time - pattern_start_time)\n}\n\n\npattern_data <- data.frame((1:length(pattern_times))*10,pattern_times)\ncolnames(pattern_data)[1] <- \"pattern_amount\"\n\nggplot(data = pattern_data, aes(x = pattern_amount,y= pattern_times)) + geom_point()\n```\n:::\n\n\nLooking at this data it seems as if there is a vague exponential distribution although the main takeaway is that finding the pattern information takes much more time than the individual queries. This would make it a much more limiting factor in my wild attempt for insane amounts of data. If I were supposedly to get the pattern data for the one million pattern goal in 160 pattern chunks then the process would take about 6,250 minutes or 100 hours. This is a problem. Even 100,000 patterns would be 10 hours and very problematic. I may have to brick my computer and soley have it do this task for a day. Or do some cloud computing. We'll have to see but for now I will only work with a subset of 1000 patterns.\n\n#### Pattern optimization\n\nUsing the data from before we can do a quick test to see which point has the optimal ration of amount to time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#pattern_data$pattern_ratio = pattern_data$pattern_times / pattern_data$pattern_amount\n\n#plot(pattern_data$pattern_ratio)\n```\n:::\n\n\nWith the smaller amount of data I have \n\n#### Pattern collection through iterative process\n\nThe next step after querying as much data as possible was using the built in get_patterns() function to get the pattern details I was interested in. The get_patterns function errors out much faster than the query function but luckily breaking it into chunks is easier as the dataframe is already given. As it turns out, appending data ends up taking longer and longer in R so I'm going to cut to the chase on some data cleaning and carve out some of the larger pieces of data i won't be using. I'm not sure if this will help or hurt speed though. I also need to write to JSON instead of csv at this point due to nested values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npatterns_start_time <- Sys.time()\nfor(i in 0:99){\n  print((i+1)*10 + ((i+1)*10))\n  pattern_inst <- get_patterns(ids = search_results$id[((i*10)+1):((i+1)*10)])\n  if(i == 0){\n    pattern_data = pattern_inst\n  } else if(i == 1){\n    pattern_data <- rbind(pattern_data,pattern_inst)\n  } else{\n    pattern_data <- rbind(pattern_data,pattern_inst)\n  }\n}\npatterns_end_time <- Sys.time()\n\n## 1.5 minutes for 1000 entries\n\nprint(patterns_end_time-patterns_start_time)\n\n\n  \n\n\nlibrary(jsonlite)\npatterns_json <- toJSON(pattern_data)\nvalidate(patterns_json)\nwrite_json(patterns_json,\"../data/pattern_data_raw.JSON\")\n```\n:::\n\n\n#### Summary of exising data\n\nI'm hoping to optimize this process, going forward I'll use a lower amount of data to do a proof of concept data cleaning step without the full data I hope to analyze. Here is a quick sense of what we're working with. Its a bit messy as a JSON but the data cleaning should improve the outcome and look.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#patterns_json <- read_json(\"../data/pattern_data_raw.JSON\")\n#prettify(patterns_json[[1]])\n```\n:::\n\n\n\n#### Conclusion\n\nOverall, I was left with a csv of raw pattern data with n entries. This process was a path of references and interest in order to collect the data. Next I will be processing the data by stripping unwanted variables and splitting tuples that exist within the dataframe structure. In the future I will continue to update my query optimization process to not overload the system and be able to maximize the amount of documents I recieve in the process\n\n\n\n",
    "supporting": [
      "data_gathering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}