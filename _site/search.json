[
  {
    "objectID": "naive_bayes/naive_bayes.html",
    "href": "naive_bayes/naive_bayes.html",
    "title": "Naive Bayes Classifier",
    "section": "",
    "text": "Code\nimport pandas as pd"
  },
  {
    "objectID": "naive_bayes/naive_bayes.html#prepping-data",
    "href": "naive_bayes/naive_bayes.html#prepping-data",
    "title": "Naive Bayes Classifier",
    "section": "Prepping Data",
    "text": "Prepping Data\n\nRecord Data\nAs mentioned earlier, my data is labeled with the type of pattern it is and so cleaning is a simple step of separating the impactful values from the label. For future reference, I’m hoping to clear up some of the missing values as their pressence reduces the effectiveness of the model.\n\n\nCode\ndf = pd.read_csv(\"../data/pattern_data_updated.csv\")\n\n\nC:\\Users\\duckd\\AppData\\Local\\Temp\\ipykernel_29272\\2793938783.py:1: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"../data/pattern_data_updated.csv\")\n\n\n\n\nCode\ndf.isnull().sum()\n\n\nUnnamed: 0                       0\ncomments_count                   0\ncurrency                     17866\nprice                            0\ncurrency_symbol              17878\nfree                             1\npublished                     4378\ndifficulty_average               0\nfavorites_count                  0\nprojects_count                   0\nqueued_projects_count            0\ngauge                        15897\ngauge_divisor                 2793\ngauge_pattern                28548\nrow_gauge                    25825\nid                               0\nname                             0\npermalink                        0\nrating_average                   0\nrating_count                     0\nyardage                      11931\nyardage_max                  11931\nyarn_weight_description          0\nyarn_weight                  96309\nneedle_sizes_us              12844\nneedle_sizes_metric              0\nhook_size                    24860\nneedle_sizes_name                0\ncraft                            0\npattern_type                     0\npattern_desc                  2120\ncategory_permalink               3\ncategory_parent_permalink        3\nprice_adjusted                   0\ndtype: int64\n\n\n\n\nCode\nx = df[[\"price\",\n        \"free\",\n        \"difficulty_average\",\n        \"gauge\",\"gauge_divisor\",\"row_gauge\",\n        \"rating_average\",\"rating_count\",\n        \"yardage\",\"yardage_max\",\n        \"yarn_weight_description\",\n        \"needle_sizes_metric\",\n        \"pattern_type\"]].dropna()\ny = x[\"pattern_type\"]\nx.drop(\"pattern_type\",inplace = True,axis = 1)\nx[\"free\"] = x[\"free\"].astype(\"category\")\n\n\nAfter clarifying my feature data (x) and my target data (y), I now have representative data for the naive bayes model I hope to use. I have also imported the sklearn test train split in the code below. I decided a 80-20 split was appropriate as its industry standard and allows for some wiggle room if more testing is needed.\n\n\nFeature Selection\nSo I will start by writing a function for running a NB test and then applying my own data to the process. I gave it a few test runs with different types of models after importing the correct packages and executing the function.\nThe function itself takes in the label and feature data, splits it into train and test sets, fits the training data to the model, tests it on the remaining data, and then compares the results with the correct options; this leaves an accuracy score calculable.\n\n\nCode\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport time\nimport numpy as np\n\n\ndef train_NB_model(X,Y,i_print=False, model_type = \"multinomial\"):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = .2,random_state = 1)\n\n    # INITIALIZE MODEL \n    if model_type == \"categorical\":\n        model = CategoricalNB()\n    elif model_type == \"multinomial\":\n        model = MultinomialNB()\n    elif model_type == \"bernoulli\":\n        model = BernoulliNB()\n    elif model_type == \"complement\":\n        model = ComplementNB()\n    elif model_type == \"gaussian\":\n        model = GaussianNB()\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test)\n\n    return (acc_train,acc_test)\n\ntest_types = [\"categorical\",\"multinomial\",\"bernoulli\",\"complement\",\"gaussian\"]\n\n#TEST\nfor i in range(0,len(test_types)):\n    print(test_types[i])\n    (acc_train,acc_test)=train_NB_model(x[[\"yardage\",\"yardage_max\",\"needle_sizes_metric\",\"gauge\"]],y,i_print = True,model_type = test_types[i])\n\n\ncategorical\n(66154, 4) (66154,)\n49.16954821155263 45.506764416899706\nmultinomial\n(66154, 4) (66154,)\n33.54307201027908 33.85231652936286\nbernoulli\n(66154, 4) (66154,)\n16.714849876235284 16.748545083515985\ncomplement\n(66154, 4) (66154,)\n36.62679742267067 36.37669110422493\ngaussian\n(66154, 4) (66154,)\n43.174045311112366 43.3905222583327\n\n\nThe results are printed out with the train accuracy first and the test accuracy second. Above it the shape of the dataframe is listed although it is the same for all the data.\nAs the quick test shows, the categorical model seems to predict the best out of all of them but the gaussian is more consistent. Some of the features are categorical like but more are numeric so I’m thinking of using both and comparing the success rates. I will move on to doinng a feature test with the two models to see what the highest accuracy rate I can get is. Otherwise we now have a functional equation and a subtype of the naive bayes classifier so I’m ready to try out some of the subsets to see what works best. I’ll be using the gaussian model first and categorical second.\n\n\nCode\nimport itertools\n\nsubsets = []\nlist1 = [*range(1,x.shape[1])]; #print(list1)\n\ntrain_accs_g = []\ntest_accs_g = []\nnum_features_g = []\nbest_test_acc_g = 0\n\n# x.shape[1]+1\nfor l in range(1,x.shape[1]+1):\n    for subset in itertools.combinations(x.columns, l):\n        train_acc, test_acc = train_NB_model(x.loc[:,list(subset)],y,model_type=\"gaussian\")\n        train_accs_g.append(train_acc)\n        test_accs_g.append(test_acc)\n        num_features_g.append(len(list(subset)))\n        if test_acc &gt; best_test_acc_g:\n            best_test_acc_g = test_acc\n            best_subset_g = list(subset)\n\nprint(best_subset_g)\n\n\n\n['gauge', 'rating_average', 'yardage_max', 'yarn_weight_description', 'needle_sizes_metric']\n\n\n\n\nCode\ntrain_acc, test_acc = train_NB_model(x.loc[:,best_subset_g],y,i_print=False,model_type = \"gaussian\")\nprint(str(train_acc) + \" \"+ str(test_acc))\n\n\n47.83742418230259 45.17421207769632\n\n\n\n\nCode\nimport itertools\n\nsubsets = []\nlist1 = [*range(1,x.shape[1])]; #print(list1)\n\ntrain_accs_c = []\ntest_accs_c = []\nnum_features_c = []\nbest_test_acc_c = 0\n\n# x.shape[1]+1\nfor l in range(1,x.shape[1]+1):\n    for subset in itertools.combinations(x.columns, l):\n        train_acc, test_acc = train_NB_model(x.loc[:,list(subset)],y,model_type=\"categorical\")\n        train_accs_c.append(train_acc)\n        test_accs_c.append(test_acc)\n        num_features_c.append(len(list(subset)))\n        if test_acc &gt; best_test_acc_c:\n            best_test_acc_c = test_acc\n            best_subset_c = list(subset)\n\nprint(best_subset_c)\n\n\n['price', 'difficulty_average', 'gauge', 'gauge_divisor', 'row_gauge', 'yardage', 'yardage_max', 'yarn_weight_description', 'needle_sizes_metric']\n\n\n\n\nCode\ntrain_acc, test_acc = train_NB_model(x.loc[:,best_subset_c],y,i_print=False,model_type = \"categorical\")\nprint(str(train_acc) + \" \"+ str(test_acc))\n\n\n49.9707121667328 47.00324994331494\n\n\nWe were left with two different best sets with the categorical set being much larger with more features and a slightly better accuracy. These are interesting results and what honestly was to be expected. The different variables are treated differently in the models so it is mainly more interesting to see that more variables make sense categorically than numerically.\nAnyhow, below is a mapping of the amount of features by accuracy. There are discrete amounts of features possible but some were more impactful than the others which lead to within subset length hierarchies.\n\n\nCode\nimport matplotlib.pyplot as plt \n\n## Gaussian distribution\nplt.plot(num_features_g,train_accs_g,'-or')\nplt.plot(num_features_g,test_accs_g,'-ob')\nplt.xlabel('Number of features')\nplt.ylabel('ACCURACY: Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.plot(num_features_c,train_accs_c,'-or')\nplt.plot(num_features_c,test_accs_c,'-ob')\nplt.xlabel('Number of features')\nplt.ylabel('ACCURACY: Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\nIt can be seen that for the gaussian naive bayes the test set does as well as if not better than the training set while that relationship is the opposite with the categorical naive bayes. This is an interesting result considering the data and shows that, while categorical methods may give better accuracy, the gaussian model works as a better fir for predicting thed ata..\n\n\nResults\nBetween the two models we have accuracy scores of around 44% and 47% for test data sets which is much lower than we would want in a commercial setting. However, this accuracy score is just one way to understand the success of the model but with some many target classes it is the easiest to understand. A confusion matrix would be harder to interpret and thus I decided to simplify the results for ease of comparability. I will be doing the same for other classifying model.\nI don’t believe my model is over or under fitted at the moment as the training accuracy is remarkably close to the test accuracy. This low accuracy is not wildly surprising due to the sparsity of the data and the complexity of the labels. from the data there are dozens of labels that crisscross and overlap across many different values. Knitting and crocheting patterns for different objects may overlap in similar needle sizes or amount of yarn so all things considered a 44 percent accuracy is still solid. I’m curious to compare this to the other classification models and I think its interesting how it does compare to other methods in efficiency.\n\n\nNaive Bayes with Labeled Text Data\nI have made a starting attempt with the text data I have available but I run into the issue of ironically having too much data. The function requires a numpy array but the array would be 170 GB with the data that I have available. I tried to assuage this as was succesful by having a minimum amount of times a word would show up and getting ride of words that did not show up as much. The data went from having 260,000 features to 10643 which is fairly drastic but makes the analysis possible in the first place. This may end up limiting the analysis of key words but I am curious to see what the results will be with it as it is.\n\n\nCode\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nintermediary = df[[\"pattern_type\",\"pattern_desc\"]].dropna()\npattern_text_data = intermediary[\"pattern_desc\"]\n\n## Testing one of the descriptions\n# print(pattern_text_data[0])\n\n## vectorizing the values\nvectorizer = CountVectorizer(min_df=.0007)\nvectorized_data = vectorizer.fit_transform(pattern_text_data)\nprint(str(vectorized_data.shape[0]) + \" by \"+ str(vectorized_data.shape[1]))\nvectorized_data = np.array(vectorized_data.todense())\n\n##one hot vecotr?\nmaxs = np.max(vectorized_data,axis = 0)\nX = np.ceil(vectorized_data/maxs)\ny = intermediary[[\"pattern_type\"]]\n\nprint(X.shape,y.shape)\n\n\n94189 by 10643\n(94189, 10643) (94189, 1)\n\n\n\n\nCode\n\nprint(\"Multinomial\")\ntrain_acc, test_acc = train_NB_model(X,y,model_type = \"multinomial\")\nprint(train_acc,test_acc)\n\n\nMultinomial\n63.515235162968466\n\n\nc:\\Users\\duckd\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nThis testing accuracy is probably the best result we’ve gotten in terms of predictive ability but it is surprisingly low considering for most descriptions people say what the pattern is and give a ton of information about it. Some of this may be due to the limitations I made in order to progress with the data. Having more network power will overcome this hopefully. I’m looking forward to the chance to use this modeling approach to better predict information."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN-5000: Personal Project Powell Sheagren",
    "section": "",
    "text": "Welcome to Powell Sheagren’s DSAN 5000 project website. Please navigate to one of the operational tabs. At the moment that includes:\nAbout Me: a summary of the author and the work that he does\nCode: a link to the author’s github\nData: a link to Ravelry where the data was pulled from (account it required)\nIntroduction: a brief intro to the data and the questions to be answered by the project\nData Gathering: the process of collecting the data from Ravelry\nData Cleaning: the process of cleaning the data for use\nData Exploration: learning about the data and deriving insights\nNaive Bayes: a classification model to predict pattern type\nClustering: a unsupervised learning method to find hidden groupings in the data\nDimensionality Reduction: reduce the dimensions of the data for better analysis\nDecision Trees: a model used predict pattern type in a different way\nConclusion: a summary of the other tabs and show of all accomplished\n\nI hope you enjoy the analysis and if there are any questions I can be reached at prs64@georgetown.edu\n\n\n\n“Word Cloud of Pattern Types”"
  },
  {
    "objectID": "dimensionality_reduction/dimensionality_reduction.html",
    "href": "dimensionality_reduction/dimensionality_reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "So, for the dimensionality reduction section I will once again be using the Ravely dataset I’ve been working on in order to create an index for popularity of a pattern. There are a couple measures I’ve looked at before such as number of comments, favorites, projects, and ratings all which point towards popularity but each don’t have a whole story.\nI will be using scikit learn packages with both the PCA and T-SNE properties attached to them. The methods are fairly self-contained so I don’t expect to need many other packages other than the basics like numpy, pandas, and matplotlib."
  },
  {
    "objectID": "dimensionality_reduction/dimensionality_reduction.html#project-proposal",
    "href": "dimensionality_reduction/dimensionality_reduction.html#project-proposal",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "So, for the dimensionality reduction section I will once again be using the Ravely dataset I’ve been working on in order to create an index for popularity of a pattern. There are a couple measures I’ve looked at before such as number of comments, favorites, projects, and ratings all which point towards popularity but each don’t have a whole story.\nI will be using scikit learn packages with both the PCA and T-SNE properties attached to them. The methods are fairly self-contained so I don’t expect to need many other packages other than the basics like numpy, pandas, and matplotlib."
  },
  {
    "objectID": "dimensionality_reduction/dimensionality_reduction.html#principal-component-analysis",
    "href": "dimensionality_reduction/dimensionality_reduction.html#principal-component-analysis",
    "title": "Dimensionality Reduction",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\nI will be establishing the data for the analysis, creating pair plots to see correlation, running a component analysis, visualizing increases in explained variance using an area plot, and visualizing the data on the first couple of components.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\npattern_df = pd.read_csv(\"../data/pattern_data_updated.csv\")\n\n\n\n\nCode\ndf_popularity = pattern_df[[\"comments_count\",\"favorites_count\",\"projects_count\",\"queued_projects_count\",\"rating_count\",\"rating_average\"]]\n\n## this is okay cause a null rating count means it hasn't been rated yet\ndf_popularity = df_popularity.fillna(0)\n\ndf_popularity.info()\n\nsns.pairplot(df_popularity)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 96309 entries, 0 to 96308\nData columns (total 6 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   comments_count         96309 non-null  int64  \n 1   favorites_count        96309 non-null  int64  \n 2   projects_count         96309 non-null  int64  \n 3   queued_projects_count  96309 non-null  int64  \n 4   rating_count           96309 non-null  int64  \n 5   rating_average         96309 non-null  float64\ndtypes: float64(1), int64(5)\nmemory usage: 4.4 MB\n\n\n\n\n\nThere definitely seems to be some linear behavior between project counts and favorites as well as ratings and counts. This would be a good sign for a regression model let alone a Principal Component Analysis approach so I’m looking forward to our results.\n\n\nCode\n## running PCA\ndf_popularity = pattern_df[[\"comments_count\",\"favorites_count\",\"projects_count\",\"queued_projects_count\",\"rating_count\",\"rating_average\"]]\n\n## this is okay cause a null rating count means it hasn't been rated yet\ndf_popularity = df_popularity.fillna(0)\n\nfrom sklearn.decomposition import PCA\nn = 5\npca = PCA(n_components=n)\npca.fit(df_popularity)\nprint('\\nPCA')\n\nfor i in range(0,n):\n    print(\"\\nPCA components:\")\n    print(pca.components_[[i]])\n    print(\"PCA explained variance:\")\n    print(pca.explained_variance_ratio_[i])\n\n\n\nPCA\n\nPCA components:\n[[3.21895085e-03 9.79748260e-01 1.37850803e-01 1.39657353e-01\n  3.96980251e-02 1.79585923e-04]]\nPCA explained variance:\n0.9816818513379555\n\nPCA components:\n[[-9.98049192e-06 -1.65318331e-01  9.38926887e-01  1.60301921e-01\n   2.55713078e-01 -5.16136294e-04]]\nPCA explained variance:\n0.016759553546169\n\nPCA components:\n[[ 0.00101344 -0.11158473 -0.21643561  0.9625503   0.11916108 -0.00100661]]\nPCA explained variance:\n0.001435235907577648\n\nPCA components:\n[[-0.02558237  0.01746727 -0.22919575 -0.16810798  0.9582444   0.00413871]]\nPCA explained variance:\n0.00010434441598611692\n\nPCA components:\n[[ 0.99961428 -0.00260078 -0.00606786 -0.00570794  0.02423672  0.01037805]]\nPCA explained variance:\n1.8569747238803367e-05\n\n\n\n\nCode\n## I'm using some visualizations for understanding from the following website:\n# https://plotly.com/python/pca-visualization/\nimport plotly.express as px\n\nexp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\npx.area(\n    x=range(1, exp_var_cumul.shape[0] + 1),\n    y=exp_var_cumul,\n    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"}\n)\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nThe figures were having trouble rendering so I am adding a image in here:\n\n\n\n“Image of increased explained variance over #components”\n\n\n\n\nCode\nfeatures = df_popularity.columns\n\nimport plotly.express as px\n\ncomponents = pca.fit_transform(df_popularity)\n\nloadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n\nfig = px.scatter(components, x=0, y=1)\n\nfor i, feature in enumerate(features):\n    fig.add_annotation(\n        ax=0, ay=0,\n        axref=\"x\", ayref=\"y\",\n        x=loadings[i, 0],\n        y=loadings[i, 1],\n        showarrow=True,\n        arrowsize=2,\n        arrowhead=2,\n        xanchor=\"right\",\n        yanchor=\"top\"\n    )\n    fig.add_annotation(\n        x=loadings[i, 0],\n        y=loadings[i, 1],\n        ax=0, ay=0,\n        xanchor=\"center\",\n        yanchor=\"bottom\",\n        text=feature,\n        yshift=5,\n    )\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\n“Plot of components of data”\n\n\nThe results of the PCA show a really strong first component which can detect most of the variance. This is good in my case and expected since most of the variables are highly correlated as seen in the pair plot above. The last visualization isn’t wildly helpful as the dimensions of the data are fairly one note and the arrows close together due to the size of the data. Now let’s apply the components to the data and see which pattern is the most popular.\n\n\nCode\nprimary_c = pca.components_[0]\n\npopularity_tally = []\nfor i in range(0,df_popularity.shape[0]):\n    rowsum = 0\n    for g in range(0,5):\n        rowsum = rowsum + (primary_c[g]*df_popularity.iloc[i,g])\n    popularity_tally.append(rowsum)\n\nprint(popularity_tally[0:10])\npopularity_tally = pd.DataFrame(popularity_tally)\n\n\n[762.9659133011417, 701.6150108970547, 606.8493714140815, 253.0165335569481, 503.42293460566384, 57.240417793649456, 62.14311242900399, 600.0041196829263, 138.38090774266988, 50.94476272476789]\n\n\n\n\nCode\ncomponents_df = pd.DataFrame(components)\npopularity_score = components_df.sum(axis = 1)\n\npop_score = pd.concat([pattern_df[\"name\"],popularity_score], axis = 1)\npop_score.rename(columns = {0:\"score\"},inplace = True)\n\npop_score = pd.concat([pop_score,popularity_tally], axis = 1)\npop_score.rename(columns = {0:\"tally\"},inplace = True)\n\nprint(pop_score.sort_values(\"score\").head())\nprint(pop_score.sort_values(\"score\").tail())\n\n\n                                   name        score     tally\n14555               Storm Warming Toque -1300.871745  0.137851\n50542  Furballs' Ticky Tabby Hat 蜱斑大花貓帽 -1300.871745  0.137851\n15303      Malizia-Roses Baby Hat (#34) -1300.871745  0.137851\n34483                     Baby Doll Hat -1300.871745  0.137851\n79019            Super Bulky HDC Beanie -1300.871745  0.137851\n                          name          score         tally\n75               The Weekender   89486.149212  89960.159960\n77                  Ranunculus   90336.365962  85476.080862\n74                        Flax   91899.319123  80512.648496\n448                 Honey Cowl   93646.231565  76507.825359\n151  Hermione's Everyday Socks  109075.391802  87302.574573\n\n\nThere are around 26 patterns with the absolute lowest score of -1300.154034 in terms of the PCA transformation score. I wasn’t sure what that function to get the scores was doing so I did a test by hand using just the first principal component and got a very similar tally for most of the numbers. The top five are pretty much the same with Hermoine’s Everyday Socks being the objectively most popular pattern and storm warning toque and the other 25 being the least popular. This is funny because this sock pattern is just very average and seems to just have caught on by being a least common denominator sock pattern."
  },
  {
    "objectID": "dimensionality_reduction/dimensionality_reduction.html#tsne",
    "href": "dimensionality_reduction/dimensionality_reduction.html#tsne",
    "title": "Dimensionality Reduction",
    "section": "TSNE",
    "text": "TSNE\nI don’t know if this will work as well for my goals as this method is more about visualization. I may try a couple processes to see if there is a perplexity which makes more sense but for now it is a dry run to see if the method is possible to use with the data.\nI will take a few fewer steps for this analysis and just run the model and visualize the results.\n\n\nCode\nfrom sklearn.manifold import TSNE\n\nprint(df_popularity.shape)\n\ndf_embedded = TSNE(n_components=3, learning_rate='auto',init='random', perplexity=30).fit_transform(df_popularity)\n\n\n(96309, 6)\n\n\n\n\nCode\n\nprint(\"RESULTS\") \nprint(\"shape : \",df_embedded.shape)\nprint(\"First few points : \\n\",df_embedded[0:2,:])\n\nplt.scatter(df_embedded[:,0],df_embedded[:,1], alpha=0.5)\nplt.show()\n\n\nRESULTS\nshape :  (96309, 3)\nFirst few points : \n [[  0.72771114 -11.23116      7.6040716 ]\n [-13.847262   -26.378683    13.738866  ]]\n\n\n\n\n\nThis and other visualizations were not nearly as helpful as hoped. It visualizes the data but does not provide much insight."
  },
  {
    "objectID": "dimensionality_reduction/dimensionality_reduction.html#project-report",
    "href": "dimensionality_reduction/dimensionality_reduction.html#project-report",
    "title": "Dimensionality Reduction",
    "section": "Project Report",
    "text": "Project Report\nI will mostly be mentioning visualizations and values from the previous sections so there will not be much added information here.\nSo as mentioned before the steps I used followed the structure of the models; running a PCA analysis and looking through the components as well as visualizing the dimensionality reduction with TSNE. The results of both of those were weights for the different variables for the former and a map that looks something like Antarctica on the latter.\nMy analysis of the PCA is that the primary component seems like the one I should use for the index as it represents the vast majority of the variability in the data and I’m sure it will be successful in finding the most popular pattern when I apply it to the data. I think the strong performance of this component is due to the preexisting correlation, which makes sense as if a pattern has more favorites then it’s more likely to be made into a project or commented on and vice versa with the whole mess.\nMy analysis of the TSNE is harder to clarify as the graph I had was not as helpful in discerning the true nature of the data. I’m not sure if this method will be as helpful since I’m not looking for groups in the data but rather a linear trend of popularity. I do think its cool that the map looks like Antarctica though.\nAnyhow, I think the PCA had a stronger use case but the TNSE was an interesting model to compare and adapt to."
  },
  {
    "objectID": "data_gathering/data_gathering.html",
    "href": "data_gathering/data_gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(reticulate)\nlibrary(ggplot2)\nlibrary(jsonlite)"
  },
  {
    "objectID": "data_gathering/data_gathering.html#initial-attempt",
    "href": "data_gathering/data_gathering.html#initial-attempt",
    "title": "Data Gathering",
    "section": "Initial Attempt",
    "text": "Initial Attempt\nAfter giving some general details about how API’s work she provided example code which was as follows. I input one of the patterns I’m a fan of to give an example response value.\n\n\nCode\n## I'm working on it\nimport http.client\nimport json\nimport requests\nimport pandas as pd\n\n## borrowed code\nauthUsername = \"read-e0c523a2ded312cf2cf7cb481460fd47\"\nauthPassword = \"oscwhYXQaBG/eh+XNej+Z50xvHiQkjPb6IrzTImB\"\n\n#define URL for the API request\nurl = 'https://api.ravelry.com/patterns/search.json?query={}&page={}&page_size={}'.format(\"Seafaring Scarf\", 1, 1, )\n#make the request\nr1 = requests.get(url, auth=requests.auth.HTTPBasicAuth(authUsername, authPassword))\n#close the connection\nr1.close()\n\nprint(\"response output formatted:\")\nprint(json.dumps(json.loads(r1.text), indent=4)) #makes the json more readable\n\n\nresponse output formatted:\n{\n    \"patterns\": [\n        {\n            \"free\": true,\n            \"id\": 968210,\n            \"name\": \"Seafaring Scarf in Big Good Wool\",\n            \"permalink\": \"seafaring-scarf-in-big-good-wool\",\n            \"personal_attributes\": null,\n            \"first_photo\": {\n                \"id\": 87973822,\n                \"sort_order\": 1,\n                \"user_id\": 31533,\n                \"x_offset\": -1,\n                \"y_offset\": -35,\n                \"square_url\": \"https://images4-f.ravelrycache.com/uploads/purlsoho/652402390/seafaring-scarf-BGW-600-4_square.jpg\",\n                \"medium_url\": \"https://images4-f.ravelrycache.com/uploads/purlsoho/652402390/seafaring-scarf-BGW-600-4_medium.jpg\",\n                \"thumbnail_url\": \"https://images4-g.ravelrycache.com/uploads/purlsoho/652402390/seafaring-scarf-BGW-600-4_thumbnail.jpg\",\n                \"small_url\": \"https://images4-g.ravelrycache.com/uploads/purlsoho/652402390/seafaring-scarf-BGW-600-4_small.jpg\",\n                \"medium2_url\": \"https://images4-a.ravelrycache.com/uploads/purlsoho/652402390/seafaring-scarf-BGW-600-4_medium2.jpg\",\n                \"small2_url\": \"https://images4-f.ravelrycache.com/uploads/purlsoho/652402390/seafaring-scarf-BGW-600-4_small2.jpg\",\n                \"caption\": null,\n                \"caption_html\": null,\n                \"copyright_holder\": null\n            },\n            \"designer\": {\n                \"crochet_pattern_count\": 48,\n                \"favorites_count\": 23745,\n                \"id\": 40686,\n                \"knitting_pattern_count\": 675,\n                \"name\": \"Purl Soho\",\n                \"patterns_count\": 723,\n                \"permalink\": \"purl-soho\",\n                \"users\": [\n                    {\n                        \"id\": 31533,\n                        \"username\": \"purlsoho\",\n                        \"tiny_photo_url\": \"https://avatars-d.ravelrycache.com/purlsoho/615839432/purl_soho_logo_tiny.jpg\",\n                        \"small_photo_url\": \"https://avatars-d.ravelrycache.com/purlsoho/615839432/purl_soho_logo_small.jpg\",\n                        \"photo_url\": \"https://avatars-d.ravelrycache.com/purlsoho/615839432/purl_soho_logo_large.jpg\"\n                    }\n                ]\n            },\n            \"pattern_author\": {\n                \"crochet_pattern_count\": 48,\n                \"favorites_count\": 23745,\n                \"id\": 40686,\n                \"knitting_pattern_count\": 675,\n                \"name\": \"Purl Soho\",\n                \"patterns_count\": 723,\n                \"permalink\": \"purl-soho\",\n                \"users\": [\n                    {\n                        \"id\": 31533,\n                        \"username\": \"purlsoho\",\n                        \"tiny_photo_url\": \"https://avatars-d.ravelrycache.com/purlsoho/615839432/purl_soho_logo_tiny.jpg\",\n                        \"small_photo_url\": \"https://avatars-d.ravelrycache.com/purlsoho/615839432/purl_soho_logo_small.jpg\",\n                        \"photo_url\": \"https://avatars-d.ravelrycache.com/purlsoho/615839432/purl_soho_logo_large.jpg\"\n                    }\n                ]\n            },\n            \"pattern_sources\": [\n                {\n                    \"amazon_rating\": null,\n                    \"amazon_reviews\": null,\n                    \"amazon_sales_rank\": null,\n                    \"amazon_updated_at\": null,\n                    \"amazon_url\": null,\n                    \"approved_patterns_count\": 701,\n                    \"asin\": \"\",\n                    \"author\": \"\",\n                    \"author_pattern_author_id\": null,\n                    \"author_surname\": \"\",\n                    \"book_binding\": null,\n                    \"completed\": false,\n                    \"created_at\": \"2016/04/23 22:05:01 -0400\",\n                    \"created_by_user_id\": 433514,\n                    \"designer_pending_patterns_count\": 0,\n                    \"designer_users_count\": 0,\n                    \"editorships_count\": 0,\n                    \"favorites_count\": 2825,\n                    \"first_photo_id\": null,\n                    \"flaggings_count\": 0,\n                    \"fulfilled_by_ravelry\": false,\n                    \"has_photo\": false,\n                    \"id\": 207513,\n                    \"isbn_13\": null,\n                    \"issue\": null,\n                    \"keywords\": \"purlsoho\",\n                    \"label\": null,\n                    \"large_image_url\": null,\n                    \"last_pattern_edit\": \"2023/11/27 14:10:09 -0500\",\n                    \"link_id\": 317032,\n                    \"list_price\": null,\n                    \"lock_version\": 17,\n                    \"medium_image_url\": null,\n                    \"name\": \"Purl Soho\",\n                    \"out_of_print\": false,\n                    \"pattern_source_type_id\": 3,\n                    \"patterns_count\": 810,\n                    \"pending_patterns_count\": 109,\n                    \"periodical\": false,\n                    \"permalink\": \"purl-soho\",\n                    \"photos_permitted\": false,\n                    \"popularity\": 0.0,\n                    \"popularity_rank\": 2147483647,\n                    \"price\": null,\n                    \"publication_date\": null,\n                    \"publication_date_set\": 0,\n                    \"publication_day_set\": 0,\n                    \"publication_sort_order\": null,\n                    \"publication_year\": null,\n                    \"publisher_id\": null,\n                    \"shelf_image_path\": null,\n                    \"shelf_image_size\": null,\n                    \"small_image_url\": null,\n                    \"source_group_id\": null,\n                    \"stickies_count\": 0,\n                    \"store_id\": null,\n                    \"updated_at\": \"2021/03/27 10:07:19 -0400\",\n                    \"url\": \"http://www.purlsoho.com/\",\n                    \"work_id\": null,\n                    \"notes\": \"\"\n                }\n            ]\n        }\n    ],\n    \"paginator\": {\n        \"page_count\": 11,\n        \"page\": 1,\n        \"page_size\": 1,\n        \"results\": 11,\n        \"last_page\": 11\n    }\n}\n\n\nThis attempt was very successful and I was able to get a pull of a pattern I liked. I did notice though that this json file did not actually have all the information I was interested in. The file itself seemed to be formatted more with information about the user and its position in the website. Given this was the case I continued into the article.\nShe expanded upon the API’s methods with classes and functions, however she also linked to some other resources which included a package in R specifically designed for use with the Ravelry API. So like most programmers I decided to use another’s work for my own benefit.\n“Documentation for the ravelRy package”"
  },
  {
    "objectID": "data_gathering/data_gathering.html#successful-attempt",
    "href": "data_gathering/data_gathering.html#successful-attempt",
    "title": "Data Gathering",
    "section": "Successful Attempt",
    "text": "Successful Attempt\nTable of process - Package overview\n- Description of process\n- Query step   - Query timing\n- Query optimization\n- Query timing\n- Optimizing by page number\n- Data collection through Query\n- Get pattern step\n- Pattern timing\n- Pattern optimization\n- Pattern collection through iterative process\n- Summary of remaining data\n- Conclusions\n\n\n\nCode\n# Never do work someone has done before\nlibrary(ravelRy)\n\"ravelRy is an R package that provides access to the Ravelry API.\"\n\"Ravelry describes itself as a place for knitters, crocheters, designers, spinners, weavers and dyers to keep track of their yarn, tools, project and pattern information, and look to others for ideas and inspiration.\"\n\n\n'ravelRy is an R package that provides access to the Ravelry API.'\n\n\n'Ravelry describes itself as a place for knitters, crocheters, designers, spinners, weavers and dyers to keep track of their yarn, tools, project and pattern information, and look to others for ideas and inspiration.'\n\n\n\nPackage Overview\nThe package required the same username and password I had made before, so I established them in my .Renviron folder so they could be accessed without console intervention. This step was required as the package would use the console to ask for username and password which stopped Quarto from rendering.\n\n\nCode\n#Establishing Credentials\n#ravelry_auth(key = 'username') \n#ravelry_auth(key = 'password')\n\n\nHaving set it up, I was left to run my query and collect as many data entries as I could reasonably. The database itself has, on my last check, one million two hundred and sixty three thousand nine hundred and eleven patterns. While I do not need all those patterns, I’m looking to get as many as reasonable which would be at maximum one million and minimum one hundred thousand.\n\n\nDescription of Process\nOn my end this collection included two steps, the first of which involved a direct query of the database as if I were doing a search (search_patterns). I could have provided some specifics to the query such as category or type but I was more concerned with having as much information as possible at this stage.\nThe second step involved using a built-in function of the package to actually get the pattern information which I will be analyzing, this is the stuff that was not available with the traditional API pull. Interestingly this step took more time to process than the previous and I did some experimenting to optimize the steps. I will mention that and more after I complete the demo of the data gathering process."
  },
  {
    "objectID": "data_gathering/data_gathering.html#query-step",
    "href": "data_gathering/data_gathering.html#query-step",
    "title": "Data Gathering",
    "section": "Query Step",
    "text": "Query Step\nAnd thus I began the process of querying data optimally and maximally.\n\nQuery Timing\nI am starting but doing some tests on query time in comparison to amount and so I took the length step of doing a for loop for each 10 queries. There is currently an abridged amount but the pattern should hold and I will prove that once I can process better.\n\n\nCode\nquery_times &lt;- c()\nstart_time &lt;- Sys.time()\nfor(i in 1:250){\n  n_query &lt;- 10*i\n  query_start_time &lt;- Sys.time()\n  search_results &lt;- search_patterns(query = '',page_size = n_query)\n  query_results_time &lt;- Sys.time()\n  query_times &lt;- c(query_times,query_results_time - query_start_time)\n}\nend_time &lt;- Sys.time()\n\n#print(\"This test took \" + str(end_time - start_time) + \" seconds to run\")\n# this took alot of time\n\n\n: \n\n\n\n\nCode\nquery_data &lt;- data.frame((1:length(query_times))*10,query_times)\ncolnames(query_data)[1] &lt;- \"query_amount\"\n\nmodel &lt;- lm(data = query_data, query_times ~ query_amount)\nsummary(model)\n\na &lt;- model$coefficients[2] # slope\nb &lt;- model$coefficients[1] # intercept\n\nggplot(data = query_data, aes(x = query_amount,y=query_times)) +\n    geom_point() + \n    geom_smooth(method = \"lm\") +\n    xlab(\"Amount of patterns querried\") +\n    ylab(\"Time taken to query patterns (s)\") +\n    theme_classic() +\n    labs(title = \"Rate of querying\")\n\n\n\nCall:\nlm(formula = query_times ~ query_amount, data = query_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8921 -0.8584 -0.0823  0.5701  4.4548 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.1560400  0.1617484   0.965    0.336    \nquery_amount 0.0076100  0.0001117  68.113   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.275 on 248 degrees of freedom\nMultiple R-squared:  0.9493,    Adjusted R-squared:  0.9491 \nF-statistic:  4639 on 1 and 248 DF,  p-value: &lt; 2.2e-16\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nUpon visualization there is a fairly linear trend with some variance change as the query rate goes larger. This variance can probably be found in the difference in computational power in my computer moment to moment. However the linear trend, as shown by the quick regression model I computed, is a good sign that there aren’t any artificial constraints on the system to limit querying. Also given the equation found of time = .0068(amount) + .174 we can guess that querying one million patterns will take about 7600 seconds or 113 minutes which is about 2 hours. It would be optimal to just plug that amount into the search patterns feature but there seems to be an unofficial limit to the amount of patterns that can be referenced which I will talk about next.\n\n\nOptimizing by Page Number\nAfter completing a large number of queries, most often at or above 5000, there will be an error that says a json wasn’t read. This implies that there is a hard limit to how many patterns can be searched in a single query. The logical solution is to do multiple queries with a set amount. In the search_patterns function there is another variable aside from page_size, which gives the number of results to retrieve, page gives which page to start one. Now the page size on the website averages to 48, however the page size of the query function is variable and is dependent on the amount of patterns being queried. However, this works in our favor as queries starting at different pages will always go by the page number for amount, meaning we can run as many queries as desired that will not overlap by keeping the amount per page consistent and using the next page. In this example we get Weekend hat being the last pattern of a 100 pattern page 1 and the first of a 99 pattern page 2. We are intentionally creating a lag here so that we can show the overlap, this would not happen if the page sizes were the same.\n\n\nCode\npage_1 &lt;- search_patterns(query = '',page_size = 100,page = 1)\npage_2 &lt;- search_patterns(query = '',page_size = 99,page = 2)\npage_2_start &lt;- page_2$name[1]\npage_1_overlap &lt;- page_1 %&gt;% filter(name == page_2_start)\n\npage_1_overlap\n\n\n\nA tibble: 1 × 7\n\n\nfree\nid\nname\npermalink\ndesigner.id\ndesigner.name\npattern_sources\n\n\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;list&gt;\n\n\n\n\nFALSE\n1300588\nWeekend Hat\nweekend-hat-5\n92889\nPetiteKnit\n, , , , , 258 , , Mette Wendelboe Okkels , 92889 , Okkels , , FALSE , 2016/10/26 09:41:37 -0400 , 6732810 , 0 , 0 , 0 , 11144 , , 2 , FALSE , FALSE , 215774 , , , petiteknit.com, Petite Knit , , , 2023/10/25 08:38:21 -0400 , 399411 , , 14 , , PetiteKnit Ravelry Store , FALSE , 7 , 262 , 4 , FALSE , petiteknit-ravelry-store , FALSE , 0 , 2147483647 , , , 0 , 0 , , , , , , , , 9 , 59482 , 2022/05/13 01:14:08 -0400 , https://www.ravelry.com/stores/petiteknit, ,\n\n\n\n\n\n\n\nData Collection through Query\nAfter condensing a way to give a max query by page I can estimate a total amount of data of 1000000 which I will now query. Based on the equation given this is expected to take around 2 hours of time give or take a few minutes due to the added time of merging data sets between steps. I’m looking here to gather one million data entries in stages of 2000 queries, so I will be running a 400 round for loop in order to accomplish this goal.\nInevitably however I ran into a different setback, the api actually only references up to the first 100,000 patterns which limits my sample size. As seen below it will not be able to return the final JSONs. With this being the case, I decided to settle with 100,000 patterns; an amount that is below optimal but will probably save me time in the long run.\n\n\nCode\n\nquery_start_time &lt;- Sys.time()\nfor(i in 1:40){\n    print(i)\n    search_results &lt;- search_patterns(query = '',page_size = 3000,page = i)\n    if(i == 1){\n        query_total &lt;- search_results\n    } else{\n        query_total &lt;- rbind(query_total,search_results)\n    }\n}\nquery_results_time &lt;- Sys.time()\n\nprint(query_results_time - query_start_time)\n\nprint(head(query_total))\n\nwrite.csv(query_total,\"query_results.csv\")\n\n\n[1] 42\n\n\nERROR: Error: API did not return json\n\n\n\n\nCode\nquery_data &lt;- query_total\nlibrary(jsonlite)\nquery_json &lt;- toJSON(query_data)\nvalidate(query_json)\nwrite_json(query_json,\"../data/query_data_raw.JSON\")\n\n\nTRUE\n\n\nAfter getting it all I wrote the data to a JSON file so I wouldn’t have to run it again later.\nAs a part of the process I also completed the timely step of calculating the query time for each query. I may have over crashed my queries per day but I was having fun."
  },
  {
    "objectID": "data_gathering/data_gathering.html#get-pattern-step",
    "href": "data_gathering/data_gathering.html#get-pattern-step",
    "title": "Data Gathering",
    "section": "Get Pattern Step",
    "text": "Get Pattern Step\n\nPattern timing\nSimilar to the query time the pattern time tends to change with the amount of data so I will do a test run of timing for a possible optimization problem. I will again go by every 10 queries and will ignore the time taken to request the query. As this is a separate step in my real process I will not need to worry about batching it in the same amount as the queries.\n\n\nCode\npattern_times &lt;- c()\n\nsearch_results &lt;- search_patterns(query = '',page_size = 2500)\n\nfor(i in 1:250){\n  pattern_start_time &lt;- Sys.time()\n  get_patterns(id = search_results$id[1:i*10])\n  pattern_results_time &lt;- Sys.time()\n  pattern_times &lt;- c(pattern_times,pattern_results_time - pattern_start_time)\n}\n\n\n\n\nCode\n\npattern_data &lt;- data.frame((1:length(pattern_times))*10,pattern_times)\ncolnames(pattern_data)[1] &lt;- \"pattern_amount\"\n\n# When it took longer than a minute to get the patterns the response changed from seconds to minutes\n# I changed it back here to keep units\npattern_data_adapt &lt;- pattern_data\n\nfor(i in 1:250){\n    if(pattern_data_adapt[i,2] &lt; 3 & i &gt;100){\n        pattern_data_adapt[i,2] &lt;- pattern_data_adapt[i,2] * 60\n    }\n}\n\nggplot(data = pattern_data_adapt, aes(x = pattern_amount,y= pattern_times)) + geom_point()\n\n\n\n\n\nLooking at this data it seems as if there is a vague exponential distribution although the main takeaway is that finding the pattern information takes much more time than the individual queries. This would make it a much more limiting factor in my wild attempt for insane amounts of data. Luckily this trend wasn’t actually too representative of the final amount of time and it was not long at all, but I still wanted to optimize and prepare for the conversion.\n\n\nPattern Optimization\nUsing the data from before we can do a quick test to see which point has the optimal ratio of amount to time. The units in this would be patterns per second and we are looking for the maximum number out of all of the tme pattern combos.\n\n\nCode\npattern_data_adapt$pattern_ratio = pattern_data_adapt$pattern_amount/pattern_data_adapt$pattern_times\n\nmax(pattern_data_adapt$pattern_ratio)\npattern_data_adapt[1:10,]\nggplot(data = pattern_data_adapt, aes(x = pattern_amount, y = pattern_ratio)) + geom_point()\n\n\n143.933179214819\n\n\n\nA data.frame: 10 × 3\n\n\n\npattern_amount\npattern_times\npattern_ratio\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n10\n0.07591796\n131.7211\n\n\n2\n20\n0.14022303\n142.6299\n\n\n3\n30\n0.26294994\n114.0902\n\n\n4\n40\n0.30642605\n130.5372\n\n\n5\n50\n0.50406885\n99.1928\n\n\n6\n60\n0.41686010\n143.9332\n\n\n7\n70\n0.52426100\n133.5213\n\n\n8\n80\n0.63188291\n126.6057\n\n\n9\n90\n0.72974205\n123.3313\n\n\n10\n100\n0.87868285\n113.8067\n\n\n\n\n\n\n\n\nWith the data that I have it seems that a lower amount around 60-70 is actually the most efficient amount of patterns to get at a time. However, with it to make an even breakdown of the 100,000 patterns I’m going to break it into 80 pattern chunks. I’m not considering the amount of time that it takes to merge the resulting values with an rbind. I will worry about this later if I need to optimize further.\n\n\nPattern Collection Through Iterative Process\nThe next step after querying as much data as possible was using the built in get_patterns() function to get the pattern details I was interested in. If I’m patterning about 100,000 ids then in chunks of 80 with about .65 seconds a package then I will have a total time as follows:\n\\[\n\\frac{100000}{80}*.63188=789.85 \\text{ seconds or } 13.16 \\text{ minutes}\n\\]\n\n\nCode\nquery_json &lt;- read_json(\"../data/query_data_raw.JSON\")\n\n\n\n\nCode\nquery_data &lt;- fromJSON(query_json[[1]])\n\n\n\n\nCode\n\npatterns_start_time &lt;- Sys.time()\nfor(i in 0:1249){\n  ## Some patterns did not return a value, this could be do to pattern deletion so a try statement is used\n  pattern_inst &lt;- try(get_patterns(ids = query_data$id[((i*80)+1):((i+1)*80)]))\n  if(i == 0){\n    pattern_data = pattern_inst\n  } else if(i == 1){\n    pattern_data &lt;- rbind(pattern_data,pattern_inst)\n  } else{\n    pattern_data &lt;- rbind(pattern_data,pattern_inst)\n  }\n}\npatterns_end_time &lt;- Sys.time()\n\n\nprint(patterns_end_time-patterns_start_time)\n\n\nError : API did not return json\nTime difference of 5.218724 hours\n\n\nSo my estimates were wildly off, the patterns actually took around five hours in order to be unpacked, which left me with a set of data that has dimensions of 99921, 53 in comparison to the 100000, 7 of the query data. There is a difference of about 79 patterns as one of the queried pattern ids seemed to not have been present when trying to convert the ids to a pattern data. My belief is that this pattern was deleted between me querying the pattern ids and converting it into information.\n\n\nCode\ndim(pattern_data)\ndim(query_data)\n\n\n\n9992153\n\n\n\n1000007"
  },
  {
    "objectID": "data_gathering/data_gathering.html#writing-the-data-to-my-computer-using-jsonlite",
    "href": "data_gathering/data_gathering.html#writing-the-data-to-my-computer-using-jsonlite",
    "title": "Data Gathering",
    "section": "Writing the Data to my Computer using JSONlite",
    "text": "Writing the Data to my Computer using JSONlite\nAnyhow, the next step, which I thought would be easier, would be to write the file to json and then read it in on the data cleaning slide. This proved to be more difficult than I thought for multiple reasons. For one, the files took 4-5 hours to write and read each which became impossible to deal with when the read function started erroring. I re-wrote and re-read the same files and got the same error which leads me to believe there will be a similar issue with one of the patterns corrupting the json when it is being written.\nI would also like to mention that the reason I’m saving it in a JSON format is due to the presence of nested dataframes. This is because the nested dataframes have a lot of information which can’t be stored in a csv format. I’m not sure if writing to csv is faster but I don’t have as much of a choice if i’m doing the data cleaning to fix the nested dataframes in another page. So lets split off the text data and save that as a csv and then test the json limit. JSONlite does have parameters in the write and read functions to reduce the data but unnesting is half of what i do in the cleaning so I’m not going to worry about it. Also it may screw up the data and I don’t care to take the time to test it.\n\n\nCode\ncolnames(pattern_data)\n\n\n\n'comments_count''created_at''currency''difficulty_average''difficulty_count''downloadable''favorites_count''free''gauge''gauge_divisor''gauge_pattern''generally_available''has_uk_terminology''has_us_terminology''id''name''pdf_url''permalink''price''projects_count''published''queued_projects_count''rating_average''rating_count''row_gauge''updated_at''url''yardage''yardage_max''personal_attributes''sizes_available''product_id''currency_symbol''ravelry_download''download_location''pdf_in_library''volumes_in_library''gauge_description''yarn_weight_description''yardage_description''pattern_needle_sizes''notes_html''notes''languages''packs''printings''craft''pattern_categories''pattern_attributes''pattern_author''photos''pattern_type''yarn_weight'\n\n\n\n\nCode\npatterns_data_text &lt;- pattern_data %&gt;% select(c(notes_html,notes))\npatterns_data_no_text &lt;- pattern_data %&gt;% select(-c(notes_html,notes))\ncolnames(patterns_data_text)\ncolnames(patterns_data_no_text)\ndim(patterns_data_text)\ndim(patterns_data_no_text)\n\n\n\n'notes_html''notes'\n\n\n\n'comments_count''created_at''currency''difficulty_average''difficulty_count''downloadable''favorites_count''free''gauge''gauge_divisor''gauge_pattern''generally_available''has_uk_terminology''has_us_terminology''id''name''pdf_url''permalink''price''projects_count''published''queued_projects_count''rating_average''rating_count''row_gauge''updated_at''url''yardage''yardage_max''personal_attributes''sizes_available''product_id''currency_symbol''ravelry_download''download_location''pdf_in_library''volumes_in_library''gauge_description''yarn_weight_description''yardage_description''pattern_needle_sizes''languages''packs''printings''craft''pattern_categories''pattern_attributes''pattern_author''photos''pattern_type''yarn_weight'\n\n\n\n999212\n\n\n\n9992151\n\n\n\n\nCode\nwrite.csv(patterns_data_text, \"../data/patterns_data_text.csv\")\n## this did not take long enough to make it seem like the culprit for the long write time\n## also for reference the total JSON with the text data was 1.96 GB\n\n\nSo, removing the text data and then writing the rest to CSV took absolutely no time at all. This gives me a better idea of splitting off the nested dataframes I’m interested in and then writing the rest out to a CSV, leaving the nested data frames for JSON to crunch.\nI will quickly do a time check to see if there is any idea of splitting apart the dataset into chunks like I did for the queries and patterns. I still believe that splitting the nested from the non nested is the best idea but I’d like to cover my bases.\n\n\nCode\njson_times &lt;- c()\n\nfor(i in 1:10){\n  data &lt;- patterns_data_no_text[1:((i+1)*10)]\n  json_start_time &lt;- Sys.time()\n  json_test &lt;- toJSON(data)\n  write_json(json_test,\"../data/json_test.JSON\")\n  json_results_time &lt;- Sys.time()\n  json_times &lt;- c(json_times,json_results_time - json_start_time)\n}\n\n\n\nERROR: Error in `[.data.frame`(patterns_data_no_text, 1:((i + 1) * 10)): undefined columns selected\n\n\n\n\nCode\nplot(json_times)\n\n\n\n\n\nYeah that doesn’t help at all.\nAnway, the splitting of the text data was so successful that I think that I will do the same for all of the non-nested data. I’ll save all the non nested data to a csv and the nested data to a json file that hopefully will be easier to process. I will also use this chance to prune some of the nested dataframes I will not be using in my analysis. It is a bit of data cleaning but I’ve already done more work on this than I thought to it will hopefully be alright.\nI will be separating the following nested columns:\ndownload_location\npattern_needle_sizes\nlanguages\npacks\nprintings\ncraft\npattern_categories\npattern_attributes\npattern_author\nphotos\npattern_type\n\nOf which I will only keep: pattern_needle_sizes\ncraft\npattern_categories\npattern_attributes\npattern_type\n\nThe rest could be useful for future use but not for the analysis I intend to use.\n\n\nCode\npatterns_to_csv &lt;- patterns_data_no_text %&gt;% select(-c(download_location,\n    pattern_needle_sizes,\n    languages,\n    packs,\n    printings,\n    craft,\n    pattern_categories,\n    pattern_attributes,\n    pattern_author,\n    photos,\n    pattern_type))\n\npatterns_to_json &lt;- patterns_data_no_text %&gt;% select(c(pattern_needle_sizes,\n    craft,\n    pattern_categories,\n    pattern_attributes,\n    pattern_type))\n\n\n\n\nCode\nwrite_csv(patterns_to_csv,\"../data/patterns_to_csv.csv\")\n\n\nSo writing that all to csv took five seconds while the json took 5 hours. I probably should have been doing this from the start.\n\n\nCode\n\npatterns_json &lt;- toJSON(patterns_to_json)\nvalidate(patterns_json)\n#write_json(patterns_json,\"../data/pattern_data_raw.JSON\")\n\n\nTRUE\n\n\n\n\nCode\n## Here goes nothing\nwrite_json(patterns_json,\"../data/pattern_data_json.JSON\")\n\n\nAnd so after the hours it took to write the json of the full text, just doing it with the nested dataframes only took one second. I would have saved some of my sanity if I had done this from the start.\n\nSummary of Situation\nI’m hoping to optimize this process, going forward I’ll use a lower amount of data to do a proof of concept data cleaning step without the full data I hope to analyze. Here is a quick sense of what we’re working with. It’s a bit messy as a JSON but the data cleaning should improve the outcome and look.\n\n\nCode\n#patterns_json &lt;- read_json(\"../data/pattern_data_raw.JSON\")\n#prettify(patterns_json[[1]])"
  },
  {
    "objectID": "data_gathering/data_gathering.html#conclusion",
    "href": "data_gathering/data_gathering.html#conclusion",
    "title": "Data Gathering",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, I was left with a csv of raw pattern data with 99,979 entries. This process was a path of references and interest in order to collect the data. Next I will be processing the data by stripping unwanted variables and splitting tuples that exist within the dataframe structure. In the future I will continue to update my query optimization process to not overload the system and be able to maximize the amount of documents I receive in the process."
  },
  {
    "objectID": "conclusions/conclusions.html",
    "href": "conclusions/conclusions.html",
    "title": "Conclusion",
    "section": "",
    "text": "As I reflect on the work I’ve done over the semester, I’d like to do a summarization of the tabs and my success in the various models used.\n\n\nData gathering was one of the most important tasks and ended up being one of the most time consuming. I was lucky to find an R package with similar functionality to the API query process I was doing in python, but that set up the next steps of accomplishing all of the queries and formatting the data.\nMy main problem to address was the API dropping the request if I tried to gather over 5000 patterns at a time. This led me to running some time tests for different amounts of patterns and settling on a decent amount in order to get as much data without going over the limit. Furthermore, I was only ever able to access 100,000 patterns due to the way the API was structured. I could have found a way around this but 100,000 patterns was a solid amount to start with.\nGiven all the pattern IDs I still needed to collect the relevant pattern info with the package so I went through another round of time testing to find an optimal amount for that case. I did and then processed it all into a dataframe. I then split this frame into the heavier text data, csvs representing the record data, and jsons with much of the nested information.\nWith all that being set up I moved to the data cleaning step.\n\n\n\nOn the data cleaning, I began by importing the data and removing columns and values that would not be of interest for my process. This included a lot of technical details as well as information about who created the pattern, which was not needed for my expected process. That being out of the way, I moved on to splitting the nested dataframes from the imported JSON file; adjusting the specific code depending on the parameter and then rejoining the information to the main file. After that I did one categorization task and type casted the numeric columns. I renamed it all and then saved it for future use.\n\n\n\nThe exploratory data analysis was one of the most fun tabs for me as it involved delving into the specifics of a lot of knitting lore and learning some great fun facts about the data. I began with some correlation checking and data summarization which gave some graphs and statistics which set a path for me to further analyze the information. I noticed that my data was peppered with outliers and so my next task became understanding the outlier behavior, looking at the patterns, and then adjusting to see if there were real trends or if the data itself was wrong.\nThat task made up a good chunk of my EDA. I found some outliers which were not appropriate and some which were, traced back information on some users who were particularly prolific, and adjusted values so that they would be appropriate for future analysis. It was also here that I replaced NA values when it was possible and left some data missing when it was not.\nUsing that knowledge, I formulated some hypotheses and began to find some relationships in the data. I was successful at pinning a relationship between yardage and price as well as difficulty and price. I Then visualized some of the text data to isolate the words which popped up the most. The visualizations of the words were particularly interesting as they gave two different glimpses at the type of vocabulary knitters use and how we label our patterns.\n\n\n\nThe Naive Bayes tab was the first foray into supervised classification algorithms with the express goal, which was repeated later, of predicting the type of pattern based off of given parameters. For Naive Bayes this was taking a set of features, assuming independence, and then using an optimization algorithm to train the relationships In the data. I did a few rounds of this including some feature selection with two different types of the model and ended up with an accuracy rate of 45%-47% which set a benchmark for further tests. I conducted a similar test with my text data and, after shrinking it some, I got an accuracy of 63% which may have been my overall highest. It still did not feel wildly successful, though, as I thought descriptions would be very able to describe the pattern and be much more succesful overall. Again it was wildly trimmed down for my computer to process it but I do hope I can run this specific test again.\n\n\n\nFor the clustering tab, My goal was to detect if there were any underlying groups in the data that were not captured by the type or categories of the patterns. I did this with several separate algorithms but was not very successful. Most of the optimal clusters, found using the elbow method or silhouette scores, ended up being very low and simply demarcating the difference between the more plentiful patterns with fewer amounts of yardage and the few that were more like outlier in terms of yardage. I think this was my least successful tab overall although one of my graphs did look like neapolitan ice cream.\n\n\n\nFor this tab, my goal was twofold: to use PCA to find a popularity statistic across different parameters and to use TSNE to visualize what some of those patterns may be. As it turned out, I was successful in the former but not in the latter. The PCA was able to capture the variability in the data with a first principal component with 98% of the variance explained which I used to reference which pattern was truly the most successful. For TSNE however the visuals took a long time to process and never seemed to give information about the data itself even at different perplexity scores. I ended up leaving it on a visualization which I thought looked like the Antarctic flag.\n\n\n\nDecision Trees were my last tab where I again worked to classify the type of a pattern based off of the information. I did a few rounds of hyperparameter selection by finding an optimal max depth as well as the features which were able to accurately predict the targets the best. Singular decision trees were only able to get me about 46% accuracy, which was comparable to the Naive Bayes methods, so I attempted a boosting method as well. This managed to increase my accuracy to 55% which, while not as good as the text Naive Bayes, ended up being one of the most successful of all of my classifying models."
  },
  {
    "objectID": "conclusions/conclusions.html#overview-of-work",
    "href": "conclusions/conclusions.html#overview-of-work",
    "title": "Conclusion",
    "section": "",
    "text": "As I reflect on the work I’ve done over the semester, I’d like to do a summarization of the tabs and my success in the various models used.\n\n\nData gathering was one of the most important tasks and ended up being one of the most time consuming. I was lucky to find an R package with similar functionality to the API query process I was doing in python, but that set up the next steps of accomplishing all of the queries and formatting the data.\nMy main problem to address was the API dropping the request if I tried to gather over 5000 patterns at a time. This led me to running some time tests for different amounts of patterns and settling on a decent amount in order to get as much data without going over the limit. Furthermore, I was only ever able to access 100,000 patterns due to the way the API was structured. I could have found a way around this but 100,000 patterns was a solid amount to start with.\nGiven all the pattern IDs I still needed to collect the relevant pattern info with the package so I went through another round of time testing to find an optimal amount for that case. I did and then processed it all into a dataframe. I then split this frame into the heavier text data, csvs representing the record data, and jsons with much of the nested information.\nWith all that being set up I moved to the data cleaning step.\n\n\n\nOn the data cleaning, I began by importing the data and removing columns and values that would not be of interest for my process. This included a lot of technical details as well as information about who created the pattern, which was not needed for my expected process. That being out of the way, I moved on to splitting the nested dataframes from the imported JSON file; adjusting the specific code depending on the parameter and then rejoining the information to the main file. After that I did one categorization task and type casted the numeric columns. I renamed it all and then saved it for future use.\n\n\n\nThe exploratory data analysis was one of the most fun tabs for me as it involved delving into the specifics of a lot of knitting lore and learning some great fun facts about the data. I began with some correlation checking and data summarization which gave some graphs and statistics which set a path for me to further analyze the information. I noticed that my data was peppered with outliers and so my next task became understanding the outlier behavior, looking at the patterns, and then adjusting to see if there were real trends or if the data itself was wrong.\nThat task made up a good chunk of my EDA. I found some outliers which were not appropriate and some which were, traced back information on some users who were particularly prolific, and adjusted values so that they would be appropriate for future analysis. It was also here that I replaced NA values when it was possible and left some data missing when it was not.\nUsing that knowledge, I formulated some hypotheses and began to find some relationships in the data. I was successful at pinning a relationship between yardage and price as well as difficulty and price. I Then visualized some of the text data to isolate the words which popped up the most. The visualizations of the words were particularly interesting as they gave two different glimpses at the type of vocabulary knitters use and how we label our patterns.\n\n\n\nThe Naive Bayes tab was the first foray into supervised classification algorithms with the express goal, which was repeated later, of predicting the type of pattern based off of given parameters. For Naive Bayes this was taking a set of features, assuming independence, and then using an optimization algorithm to train the relationships In the data. I did a few rounds of this including some feature selection with two different types of the model and ended up with an accuracy rate of 45%-47% which set a benchmark for further tests. I conducted a similar test with my text data and, after shrinking it some, I got an accuracy of 63% which may have been my overall highest. It still did not feel wildly successful, though, as I thought descriptions would be very able to describe the pattern and be much more succesful overall. Again it was wildly trimmed down for my computer to process it but I do hope I can run this specific test again.\n\n\n\nFor the clustering tab, My goal was to detect if there were any underlying groups in the data that were not captured by the type or categories of the patterns. I did this with several separate algorithms but was not very successful. Most of the optimal clusters, found using the elbow method or silhouette scores, ended up being very low and simply demarcating the difference between the more plentiful patterns with fewer amounts of yardage and the few that were more like outlier in terms of yardage. I think this was my least successful tab overall although one of my graphs did look like neapolitan ice cream.\n\n\n\nFor this tab, my goal was twofold: to use PCA to find a popularity statistic across different parameters and to use TSNE to visualize what some of those patterns may be. As it turned out, I was successful in the former but not in the latter. The PCA was able to capture the variability in the data with a first principal component with 98% of the variance explained which I used to reference which pattern was truly the most successful. For TSNE however the visuals took a long time to process and never seemed to give information about the data itself even at different perplexity scores. I ended up leaving it on a visualization which I thought looked like the Antarctic flag.\n\n\n\nDecision Trees were my last tab where I again worked to classify the type of a pattern based off of the information. I did a few rounds of hyperparameter selection by finding an optimal max depth as well as the features which were able to accurately predict the targets the best. Singular decision trees were only able to get me about 46% accuracy, which was comparable to the Naive Bayes methods, so I attempted a boosting method as well. This managed to increase my accuracy to 55% which, while not as good as the text Naive Bayes, ended up being one of the most successful of all of my classifying models."
  },
  {
    "objectID": "conclusions/conclusions.html#reflecting-on-research-questions",
    "href": "conclusions/conclusions.html#reflecting-on-research-questions",
    "title": "Conclusion",
    "section": "Reflecting on Research Questions",
    "text": "Reflecting on Research Questions\n\nWhat relationships exist between known parameters?\n\nI was able to establish some relationships between parameters but couldn’t with others. The data was fairly dense so there is more to discover.\n\nAre there groupings outside of known parameters that better represent the data?\n\nIt did not seem like there were other than general grouping in the density of the data.\n\nAre there any parameters that overlap and could reduce redundancy?\n\nThere was no overlap other than in values with slight differences such as notes, notes_html and needle size US vs. metric.\n\nWhat sort of keywords are most often used in pattern descriptions?\n\nIntuitively, pattern was the most cited with others such as knit, yarn, gauge, needle, and size being common.\n\nWhat ways are there to measure popularity of a specific pattern\n\nI used the counts of various things and ratings scores in a Principal Component Analysis and was able to determine popularity in a cohesive way.\n\nHow could a more tailored recommendation algorithm be built?\n\nWhile I did not get to do this, a ARM model could be fitted to fill that purpose\n\nAre some pattern types more popular than others?\n\nYes! Socks, shawls, pullovers, and hats were by far more popular than all other patterns, even scarves!\n\nCould a machine learning process determine the kind of garment by the parameters?\n\nWith mixed results. Given the data, accuracy would be a coinflip but it is within possibility.\n\nHow could pattern data be best correlated with real world trends if at all?\n\nThe data is not time seriesed to allow for overlap in that way but there are some historic outliers such as the pussy cat hat mentioned in the EDA section.\n\nWhat parameters could be used to create a more succinct pattern browsing experience\n\nFrom my own knowledge of the patterns, dimensional values like gauge give an idea of shape. This could help knitters find something appropriate by accurate size and give the models another powerful datapoint to help predict."
  },
  {
    "objectID": "conclusions/conclusions.html#future-work",
    "href": "conclusions/conclusions.html#future-work",
    "title": "Conclusion",
    "section": "Future Work",
    "text": "Future Work\nWhile the data that I had was very successful in being able to accurately label, classify, visualize, and describe relationships in the data, I only scratched the surface of the database itself and the different parameters it contains.\nFirst, I was only able to access 100,000 patterns recommended by Ravelry. This was a blessing and a curse as querying more patterns may have taken exponentially longer but in the future I would like to try to have a full list of all the patterns, the users that created them, and all the information about the patterns themselves. This would be over one million lines of information and could most likely be used to improve the performance of the pre-existing models on this website.\nFurther, I did not engage directly with the pattern attributes and categories which act as subsections for users to find the patterns they were interested in. Due to the nested and recursive nature of these parameters this did not seem to be a successful goal for this project but I can imagine using those as a help in classifying in the future; or using the attributes and categories as a target value itself.\nLastly, I would like to spend more time focusing on user behavior and creating a feedback loop for Ravelry to encourage use with optimized targeting algorithms. This goal was mostly halted by the lack of patterns available in total, as I would only have a slice of each user’s interaction with the site. In the future though there may be success in creating better targeting algorithms with a method like ARM which I did not approach in this project."
  },
  {
    "objectID": "conclusions/conclusions.html#closing-thoughts",
    "href": "conclusions/conclusions.html#closing-thoughts",
    "title": "Conclusion",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nAs I said when I started this project, this has been one of my favorite datasets I’ve worked on as it relates to a personal passion and has a good mix of record and label data for me to use with models. My interest in some of the more data based parts of knitting has increased and I have learned many fun facts which I was able to share with friends, knitting circles, and the internet. I’m glad to have had the chance to work on this project and I look forward to continuing to work with the data and gain more insight into my favorite hobby."
  },
  {
    "objectID": "about_me/about_me.html",
    "href": "about_me/about_me.html",
    "title": "About Me",
    "section": "",
    "text": "Powell Sheagren is a current grad student at Georgetown University’s Data Science and Analytics program. They graduated in 2022 from Swarthmore College with a degree in Political Science and Mathematics with an Emphasis in Statistics. They are interested in a career in political data science and have pursued volunteer opportunities with groups like Blue Bonnet Data and the Voter Empowerment Project. They are excited to start volunteering again for the 2024 election cycle and hope that the democrats will pick up seats in the house.\nPowell is from San Diego where they attended Pacific Ridge College Prep School. They took an interest in math and science while being a constant in the school’s theater programs. They continued these passions in college by his majors and by acting as the co-coordinator of the Swarthmore Drama Board theater production group. While there, they also got into knitting and started the Swarthmore Upcycling, Crocheting, and Knitting club.\nOutside of school. Powell is an avid reader and movie buff, to this day they make it a point to go see a movie each month in person to keep track of the field. They are a food buff and hope to start a food blog someday.\n\n\n\n“Picture of Powell Sheagren”"
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "In this clustering section I hope to use features about patterns such as yardage, gauge, and needle sizes to understand groupings of patterns that go beyond type. My main reason for this is that I am curious about subgenres of patterns such as hats and mitts for instance which may use similar needle sizes or amounts of yarn. These categories can not be seen simply through comparison as there is likely to be a lot of overlap so I hope that utilizing clustering will help me visualize these subgroups. This may also give a value I can feed back into my naive bayes tab for improved predictive capability.\nFor more of my feature data I am going to keep things simple and use three dimensions with the size of needles visually on one axis visually, the average yardage of yarn on the other, and then yarn weight as the size of points. This will make it easier to visualize differences and I know from previous feature selection tests these variables would cover the most important aspects of the pattern. I will be keeping track of the type of pattern to use as comparison in final visualizations."
  },
  {
    "objectID": "clustering/clustering.html#introduction",
    "href": "clustering/clustering.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "In this clustering section I hope to use features about patterns such as yardage, gauge, and needle sizes to understand groupings of patterns that go beyond type. My main reason for this is that I am curious about subgenres of patterns such as hats and mitts for instance which may use similar needle sizes or amounts of yarn. These categories can not be seen simply through comparison as there is likely to be a lot of overlap so I hope that utilizing clustering will help me visualize these subgroups. This may also give a value I can feed back into my naive bayes tab for improved predictive capability.\nFor more of my feature data I am going to keep things simple and use three dimensions with the size of needles visually on one axis visually, the average yardage of yarn on the other, and then yarn weight as the size of points. This will make it easier to visualize differences and I know from previous feature selection tests these variables would cover the most important aspects of the pattern. I will be keeping track of the type of pattern to use as comparison in final visualizations."
  },
  {
    "objectID": "clustering/clustering.html#theory",
    "href": "clustering/clustering.html#theory",
    "title": "Clustering",
    "section": "Theory",
    "text": "Theory\nI will be trying out a variety of clustering methods to see which is able to capture the structure of the data best. The methods will be detailed out below for reference.\n\nK-Means Clustering\nK-means clustering is a centroid based method which endeavors to group a dataset into n clusters with each data point being attributed to the nearest centroid. The process is iterative: a centroid is chosen at random to start, points are group by closeness to each centroid, the centroids are recalculated using the mean of the established clusters, and the whole process is repeated starting at the second step and looping until there is minimal difference from one loop to the next. The goal is to minimize the within-cluster variances which is done when calculating the squared euclidean distance between each point and the centroid.\nThis approach works best for best for a gaussian data distribution although it can work at clustering in other cases. It’s an iterative approach and there may be some behavior determined by the random initial centroids so It’s important to run a few cases to understand the distribution. Additionally, you can adjust the amount of initial centroids and visualize which number of clusters minimizes that within-cluster variance. The algorithm works best with some trial and error but is a strong and reliable method to compute clusters in the data.\n\n\nDBSCAN\nDBSCAN, in contrast to K-means, is a density based method that checks for density and proximity in the data rather than being centroid based. This type of method is very helpful when there are irregularities and outliers as they use location rather than mean to group. It works by grouping points which are neighbors and then chaining these points together when their intra-group distance is lower than the distance to other points. Unlike K-means, outliers are not included and can be left outside of the calculated groups.\nDBSCAN is one of the more popular clustering methods and is well suited for non-linearly separated clusters. I’m unsure of the nature of my data so having a nonlinear option will be helpful when considering pattern density.\n\n\nHierarchical Clustering\nNow, both of the previously discussed metrics are partitioning based; hierarchical models, in contrast, focus more on creating structures of groupings depending on the level of specificity required. The way it works is that starting either from the top (division) or bottom (agglomeration), groups are formed and then divided/combined level by level until you reach the opposite of what you started with, a group with all data points or a group for each data point. The overall concept is that, like decision trees, you can decide how deep the clustering could go and compare each ‘level’ to see which will be most useful for your data.\nThis method has its benefits and drawbacks. Like K-means, it can be tweaked roughly by number of groups and by structure while also being nested so that each grouping can be further analyzed by the sub groups within it. This can be a lot of information and the descriptive statistics to see which level is best can be complicated. However, this method proves to be a strong indicator for groups and groups within groups such that it would take multiples of other models to compete with its usefulness."
  },
  {
    "objectID": "clustering/clustering.html#methods",
    "href": "clustering/clustering.html#methods",
    "title": "Clustering",
    "section": "Methods",
    "text": "Methods\nI will be using the methods listed above, this includes k-means, dbscan, and hierarchical clustering. I’ll do them in that order with some representations of the optimal groups and results shown later.\n\n\nCode\n# importing all relevent libraries for clustering. \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics import silhouette_score\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nData Selection\nI have already mentioned some of the features I will be using and so this is the process to get the variables into a data frame for analysis. I will not be using a feature selection test as I’m unsure of the reuslts I’m looking for, so instead I will limit the features to those best for visual analysis which were mentioned earlier.\n\n\nCode\npattern_df = pd.read_csv(\"../data/pattern_data_updated.csv\")\n\npattern_df.columns\n\n\nIndex(['Unnamed: 0', 'comments_count', 'currency', 'price', 'currency_symbol',\n       'free', 'published', 'difficulty_average', 'favorites_count',\n       'projects_count', 'queued_projects_count', 'gauge', 'gauge_divisor',\n       'gauge_pattern', 'row_gauge', 'id', 'name', 'permalink',\n       'rating_average', 'rating_count', 'yardage', 'yardage_max',\n       'yarn_weight_description', 'yarn_weight', 'needle_sizes_us',\n       'needle_sizes_metric', 'hook_size', 'needle_sizes_name', 'craft',\n       'pattern_type', 'pattern_desc', 'category_permalink',\n       'category_parent_permalink', 'price_adjusted'],\n      dtype='object')\n\n\n\n\nCode\nyardage_average = pattern_df[[\"yardage\",\"yardage_max\"]].mean(axis = 1)\n\nclustering_df = pattern_df[[\"needle_sizes_metric\",\"yarn_weight_description\"]]\n\nX = pd.concat([clustering_df, yardage_average,pattern_df[\"pattern_type\"]], axis = 1).dropna()\n\nX.rename(columns = {0:\"yardage_average\"},inplace = True)\n\nY = X[\"pattern_type\"]\nX = X.drop(\"pattern_type\",axis=1)\n\nprint(X.head())\nprint(Y.head())\n\n\n   needle_sizes_metric  yarn_weight_description  yardage_average\n0                 4.50                        3           1920.0\n1                 2.75                        0           1210.0\n2                 3.75                        4           1395.0\n3                 5.50                        0           2025.5\n4                 2.50                        3            683.5\n0       other\n1    pullover\n2    pullover\n3    pullover\n4       scarf\nName: pattern_type, dtype: object\n\n\n\n\nK-Means\nI’ll start with the K-means method, running an analysis and then visualizing based on the optimal hyperparameters.\n\n\nCode\n# Initializing some empty variables\nclusters = []\ndistortions = []\ninertias = []\n\n# do all the clusters\nfor i in range(1,11):\n    kmeans_inst = KMeans(n_clusters = i, random_state = 0)\n    kmeans_inst.fit(X)\n    clusters.append(i)\n    centers_inst = kmeans_inst.cluster_centers_\n    distortions.append(sum(np.min(pairwise_distances(X, centers_inst, metric='euclidean'), axis=1)) / X.shape[0])\n    inertias.append(kmeans_inst.inertia_)\n\n## collecting the lists\ndf = pd.DataFrame({'clusters': clusters, 'distortions': distortions, 'inertias': inertias})\n\nprint(df)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\n\nax[0].plot(df['clusters'], df['distortions'], marker='o')\nax[0].set_title('Distortion')\n\n# Plot Inertia\nax[1].plot(df['clusters'], df['inertias'], marker='o')\nax[1].set_title('Inertia')\n\n# Display the side-by-side plots\nplt.show()\n\n\n   clusters  distortions      inertias\n0         1   525.291772  3.758801e+10\n1         2   255.033544  1.101013e+10\n2         3   198.843836  5.770936e+09\n3         4   154.831551  3.393179e+09\n4         5   133.446701  2.368975e+09\n5         6   101.753839  1.649563e+09\n6         7    88.709757  1.205346e+09\n7         8    79.986589  9.326244e+08\n8         9    72.863267  7.557096e+08\n9        10    67.987446  6.329259e+08\n\n\n\n\n\n\n\nCode\n## Sources from this website: https://medium.com/nerd-for-tech/k-means-clustering-using-python-2150769bd0b9#:~:text=Distortion%20is%20the%20average%20sum,here%20can%20be%20Euclidean%20distance.\n\nfrom kneed import KneeLocator\nkl = KneeLocator(range(1, 11), df[\"inertias\"], curve=\"convex\", direction=\"decreasing\")\nkl.elbow\n\n\n3\n\n\nLooking at the graphs, there’s a fall off after the first cluster and the elbow determiner gave 2 clusters as the optimal amount. I’ll do a visualization of this grouping in the results section.\n\n\nDBSCAN\nFor this process we are setting a min value amount as well as an epsilon amount in order to find a cluster which best suits the data. Since the dataset I’m using is fairly large the process becomes computationally costly, so to speed up the process I will just be using the stock amount of minimum values which is 5. This will cut down processing time and still hopefully allow for meaningful results.\n\n\nCode\ntop_scores = []\nepsilons = []\nclusters = []\n\nX2 = X.iloc[0:25000,:]\nX2.shape\n\n# setting list of density groups\ndbs = [(i / 10) for i in range(5, 30)]\n\nfor i in dbs:\n    max_score = -1\n    best_cluster = -1\n    best_eps = -1\n    g = 5\n    model = DBSCAN(eps=i, min_samples=g)\n    predics = model.fit_predict(X2)\n    num_clusters = len(pd.Series(predics).unique())\n    if num_clusters &gt; 1:\n        score = silhouette_score(X2, predics)\n        if score &gt; max_score:\n            max_score = score\n            best_cluster = num_clusters\n            best_eps = i\n    top_scores.append(max_score)\n    clusters.append(best_cluster)\n    epsilons.append(best_eps)\n\ndb = pd.DataFrame({'epsilons': epsilons, 'top_clusters': clusters, 'best_silhouette': top_scores})\nprint(db.sort_values(by=\"best_silhouette\", ascending=False))\nsns.lineplot(data=db, \n             x='top_clusters',\n             y='best_silhouette')\nplt.show()\n\n\n    epsilons  top_clusters  best_silhouette\n24       2.9            70         0.400891\n23       2.8            68         0.392865\n21       2.6            80         0.369742\n22       2.7            75         0.366855\n20       2.5            91         0.222975\n7        1.2           514         0.002759\n8        1.3           484         0.000691\n9        1.4           478        -0.011884\n17       2.2           122        -0.038180\n16       2.1           123        -0.066233\n5        1.0           599        -0.067475\n14       1.9           194        -0.070816\n6        1.1           562        -0.087972\n18       2.3           103        -0.098506\n3        0.8          1043        -0.128891\n4        0.9          1043        -0.128891\n19       2.4           100        -0.158127\n2        0.7           949        -0.181661\n1        0.6           947        -0.182431\n10       1.5           321        -0.206129\n0        0.5           910        -0.208966\n15       2.0           150        -0.227145\n11       1.6           267        -0.231841\n13       1.8           252        -0.240525\n12       1.7           255        -0.266845\n\n\n\n\n\nFrom the silhouette analysis, it seems that the silhouette score was highest at 63 clusters which is very different from the k-means conclusion so I’m interested in the comparison of the two.\n\n\nHierarchical Clustering\nI’ll run a similar hierarchical clustering model to find the optimal amount of clusters on the system.\n\n\nCode\n\nsil_scores = []\nclusters = []\n\nfor i in range(2,10):\n    max_score = -1\n    best_cluster = -1\n    best_eps = -1\n    hierarchical_cluster = AgglomerativeClustering(n_clusters=i, affinity='euclidean', linkage='ward') \n    predics = hierarchical_cluster.fit_predict(X2)\n    labels = hierarchical_cluster.labels_\n    sil_scores.append(silhouette_score(X2,labels))\n    clusters.append(i)\n    \n\n\n\n\nCode\nfig, ax = plt.subplots()\nax.plot(clusters, sil_scores, \"-o\")  \nax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\nplt.show()\n\n\n\n\n\n\n\nCode\n## I had too many data points for the model, so I will be shrinking it down to just 25000 data points\nlinkage_matrix = linkage(X.iloc[0:25000,:], method='ward')\n\nplt.figure(figsize=(10, 5))\ndendrogram(linkage_matrix, orientation='top', \n           labels=labels, \n           distance_sort='ascending', \n           show_leaf_counts=True\n           )\nplt.show()\n\n\n\n\n\nIt seems that 2-3 clusters represented the data best. While 2 clusters had a better score I will be using 3 to make it different from the k-means method."
  },
  {
    "objectID": "clustering/clustering.html#results",
    "href": "clustering/clustering.html#results",
    "title": "Clustering",
    "section": "Results",
    "text": "Results\nSo given the three methods we have a variety of optimal clusters which can give us some more information on the data. For K-means we had 2, DSBSCAN said 63, and Hierarchical Clustering gave 3. I’m unsure what this means about the data but I’m going to lead with visualization in order to see the clustering behavior in the data. I’m also going to continue to use the first 25,000 variables as my computer could not allocate the memory to run the models with all my data, but the amount given should be enough.\n\nK-means\nFor Kmeans we had 2 clusters being optimal so let’s see how that value reflects on the data.\n\n\nCode\nkmeans = KMeans(n_clusters = 2, random_state = 0)\nkmeans.fit(X2)\ny_kmeans = kmeans.predict(X2)\nX_kmeans = X2\nX_kmeans['cluster'] = y_kmeans\nX_kmeans.head()\n\nsns.scatterplot(data =X_kmeans,\n                x= \"needle_sizes_metric\",\n                y = \"yardage_average\",\n                size = \"yarn_weight_description\",\n                hue = \"cluster\")\n\n\n\n&lt;Axes: xlabel='needle_sizes_metric', ylabel='yardage_average'&gt;\n\n\n\n\n\nAs expected from a two cluster data set, it seems the split does not help clarify too many groups. It does however show that patterns with more than roughly 1000 yards of fabric constitute their own group and have more in common with each other. This is reasonable as many of those patterns linger in the same range of needle size. I’m more impressed with how straight the clustering line is, it might be because the axes are uneven numerically.\n\n\nDBSCAN\nDBSCAN operates on density versus location so there may be more opportunities for subgroups in its analysis. since there are so many groups I’ll be taking a subset of the subset to see if there is more of a gradient.\n\n\nCode\ndbscan = DBSCAN(eps=2.9, min_samples=5)\ndbscan.fit(X2)\ny_dbscan = dbscan.labels_\nX_dbscan = X2\nX_dbscan['cluster'] = y_dbscan\nX_dbscan.head()\n\nsns.scatterplot(data =X_dbscan.iloc[0:1000,:],\n                x= \"needle_sizes_metric\",\n                y = \"yardage_average\",\n                size = \"yarn_weight_description\",\n                hue = \"cluster\")\n\n\n&lt;Axes: xlabel='needle_sizes_metric', ylabel='yardage_average'&gt;\n\n\n\n\n\nThe coloring of the subgroups is a gradient so it’s harder to get a full sense of the behavior of all of the clusters but it can be seen that there are groupings of data points around the 2000-2500 yardage range which are highly correlated with each other. The rest of the data might have more nuance but it seems that most of the clustering data is in that range which is interesting.\nThere could be more to see here but there is the inverse problem of the K-means clustering which is too many clusters to really get a sense for things. It doesn’t look awful but still isn’t terribly helpful in understanding the nature of the data.\n\n\nHierarchical Clustering\nHierarchical clustering brings the optimal number of clusters down to 3 and I’m interested to see if it reflects the previous two clustering results.\n\n\nCode\nhc = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward') \nhc.fit_predict(X2)\ny_hc = hc.labels_\nX_hc = X2\nX_hc['cluster'] = y_hc\nX_hc.head()\n\nsns.scatterplot(data =X_hc,\n                x= \"needle_sizes_metric\",\n                y = \"yardage_average\",\n                size = \"yarn_weight_description\",\n                hue = \"cluster\")\n\n\n&lt;Axes: xlabel='needle_sizes_metric', ylabel='yardage_average'&gt;\n\n\n\n\n\nIn a way it does, this neapolitan ice cream stack reflects some of the density of the DBSCAN and the high low yardage split of the K-means. It shows the same information roughly that shorter, medium, and high yardage patterns all become groups of their own. I would like to note here that a pattern with over 1000 yards can take up to a month for an average knitter so we could see some of these patterns grouped by timing to complete or relative degrees of intensity."
  },
  {
    "objectID": "clustering/clustering.html#conclusions",
    "href": "clustering/clustering.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions",
    "text": "Conclusions\nClustering was an interesting method to bring to the data for analysis but It seems that the models painted with too few or too many strokes for the data provided. All of the models gave results which split the data by differences in the amount of yardage without much care for the other variables. This was an interesting result in that it gives a sense that most patterns with less yardage have more in common than different even despite different needle sizes. This may also be worth coming back to with trimmed NA values to improve the total results.\nI am a little disappointed that the analysis could not find better groups covering different types of patterns but with a more streamlined dataset more results may be found. I’m glad I got the chance to run a clustering approach and I’m just going to enjoy the neapolitan ice cream coloring of the hierarchical clustering graph."
  },
  {
    "objectID": "data_cleaning/data_cleaning.html",
    "href": "data_cleaning/data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(jsonlite)\nlibrary(reticulate)\nlibrary(ravelRy)\nlibrary(IRkernel)\nlibrary(plyr)\nlibrary(dplyr)\nuse_condaenv(\"C:/Users/duckd/anaconda3/python.exe\")"
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#introduction",
    "href": "data_cleaning/data_cleaning.html#introduction",
    "title": "Data Cleaning",
    "section": "Introduction",
    "text": "Introduction\nThis tab will go through my process of cleaning and prepping the raw data which I captures on the previous data gathering tab. I go through a few different steps and methods and eventually end up with a representative data frame."
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#contents",
    "href": "data_cleaning/data_cleaning.html#contents",
    "title": "Data Cleaning",
    "section": "Contents",
    "text": "Contents\n\nInserting data\nRemoving unnecessary columns\nSplitting nested data frames\nType casting and column renaming\nConclusion"
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#inserting-data",
    "href": "data_cleaning/data_cleaning.html#inserting-data",
    "title": "Data Cleaning",
    "section": "Inserting Data",
    "text": "Inserting Data\nThis step is the easiest and just requires loading the document saved from the data collection phase and reconverting it to a data frame. There was only a slight complication in that when it reads in the file it takes the whole json and nests it in an array, so in order to convert back we have to double index. I’m also going to run a quick distinct function in case of repeated data.\n\n\nCode\n## Importing data\npatterns_nested_json &lt;- read_json(\"../data/pattern_data_json.JSON\")\npatterns_nested_df &lt;- fromJSON(patterns_nested_json[[1]])\npattern_data_csv &lt;- read.csv(\"../data/patterns_to_csv.csv\")\npattern_text_csv &lt;- read.csv(\"../data/patterns_data_text.csv\")"
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#removing-unneeded-columns",
    "href": "data_cleaning/data_cleaning.html#removing-unneeded-columns",
    "title": "Data Cleaning",
    "section": "Removing Unneeded Columns",
    "text": "Removing Unneeded Columns\nThere are many different data points which were collected with the get_patterns function. Many of them are specific to identification or excess date information about when patterns were created which I may include later but will remove for now. I’m generally looking for information about the needles and yarn needed which includes type or size on both metrics. The other information I hope to get is the pattern type which has entries such as hat, scarf, towel, dishcloth, etc. There aren’t many of those patterns which is why to get significant amounts I aim to get large amounts of data. So, on this first step I will list all of the columns and then select the ones I’m interested in.\n\n\nCode\npatterns_df &lt;- pattern_data_csv %&gt;% select(c(\"comments_count\", # metric of interest\n  \"currency\", \"price\",\"currency_symbol\",\"free\", # financial aspects\n  \"published\", # when the pattern was published\n  \"difficulty_average\", # recorded difficulty average\n  \"favorites_count\", \"projects_count\", \"queued_projects_count\", # other metrics of interest\n  \"gauge\", \"gauge_divisor\", \"gauge_pattern\",\"row_gauge\", # technical gauge aspects\n  \"id\", \"name\", \"permalink\", # referencing information\n  \"rating_average\",\"rating_count\", # ratings information\n  \"yardage\",\"yardage_max\",\"yarn_weight_description\", # amount and weight of yarn required for pattern\n  \"yarn_weight\", # nested dataframe with weight information, could be redundant\n))\n\ndim(patterns_df)\n\n\n\n9992123\n\n\nVariables were dropped for a couple of reasons. Some had excessive amounts of irrelevant information like the photos column (dropped in data gathering) or were too internally different for analysis like the sizes_available (dropped in data gathering) which was entered differently each time. Many were booleans relating to pattern availability which was irrelevant to a detailed database. Others, lastly, were redundant information which repackaged other variables; in these cases I used the columns that already had the original data instead of the combination. This saved me a few steps here and there. Overall, I have dropped from an initial column amount of 53 to 30, however this amount may increase as I start to expand some of the nested data frames."
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#splitting-simpler-nested-dataframes",
    "href": "data_cleaning/data_cleaning.html#splitting-simpler-nested-dataframes",
    "title": "Data Cleaning",
    "section": "Splitting Simpler Nested Dataframes",
    "text": "Splitting Simpler Nested Dataframes\nOut of all of the nested dataframes there are some which just contain 3 columns of ids, values, or other without multiple rows. These I hope to go through and represent as a couple of columns. The other more dense nested dataframes will be parsed later as those often have a couple layers or many more options of data. I will start looking at instances of all the nested data frames.\n\n\nCode\nhead(patterns_nested_df)\ndim(patterns_nested_df)\npatterns_df_denest &lt;- cbind(patterns_df,patterns_nested_df,pattern_text_csv$notes)\ndim(patterns_df_denest)\n\n\n\nA data.frame: 6 × 5\n\n\n\npattern_needle_sizes\ncraft\npattern_categories\npattern_attributes\npattern_type\n\n\n\n&lt;list&gt;\n&lt;list&gt;\n&lt;list&gt;\n&lt;list&gt;\n&lt;list&gt;\n\n\n\n\n1\n7 , 7 , 4.5 , , FALSE , TRUE , , US 7 - 4.5 mm, 4.5\n2 , Knitting, knitting\n895 , Other , other-accessories, 337 , Accessories , accessories , 301 , Categories , categories\n265 , 267 , 311 , chart , written-pattern , stripes-colorwork\nFALSE, 9 , Other, other\n\n\n2\n2 , 2.0 , 2.75 , 1 , TRUE , FALSE , C , 2.75 mm (C), 2.75\n1 , Crochet, crochet\n306 , Pullover , pullover , 319 , Sweater , sweater , 302 , Clothing , clothing , 301 , Categories, categories\n62 , 114 , 150 , 265 , 267 , lace , sleeves , straight , chart , written-pattern\nTRUE , 4 , Pullover, pullover\n\n\n3\n5 , 7 , 5 , 7 , 3.75 , 4.5 , , , FALSE , FALSE , TRUE , TRUE , F , , US 5 - 3.75 mm, US 7 - 4.5 mm , 3.75 , 4.5\n2 , Knitting, knitting\n306 , Pullover , pullover , 319 , Sweater , sweater , 302 , Clothing , clothing , 301 , Categories, categories\n3 , 10 , 181 , 205 , 211 , 257 , 265 , 267 , 286 , 88 , unisex , adult , stranded , seamless , top-down , positive-ease , chart , written-pattern, in-the-round , circular-yoke\nTRUE , 4 , Pullover, pullover\n\n\n4\n9 , 9 , 5.5 , , FALSE , TRUE , I , US 9 - 5.5 mm, 5.5\n2 , Knitting, knitting\n306 , Pullover , pullover , 319 , Sweater , sweater , 302 , Clothing , clothing , 301 , Categories, categories\n1 , 3 , 9 , 10 , 64 , 66 , 91 , 150 , 204 , 205 , 211 , 257 , 267 , 285 , 286 , 311 , male , unisex , teen , adult , ribbed , stripes , boat-neck , straight , one-piece , seamless , top-down , positive-ease , written-pattern , worked-flat , in-the-round , stripes-colorwork\nTRUE , 4 , Pullover, pullover\n\n\n5\n21 , 20 , 9 , 1½ , 2½ , 9 , 2.5 , 3 , 5.5 , , , , FALSE , FALSE , FALSE , TRUE , TRUE , TRUE , , , I , US 1½ - 2.5 mm, US 2½ - 3.0 mm, US 9 - 5.5 mm, 2.5 , 3 , 5.5\n2 , Knitting, knitting\n339 , Scarf , scarf , 338 , Neck / Torso, neck-torso , 337 , Accessories , accessories , 301 , Categories , categories\n8 , 10 , 64 , 267 , 268 , 285 , child , adult , ribbed , written-pattern, video-tutorial , worked-flat\nTRUE , 1 , Scarf, scarf\n\n\n6\n7 , 7 , 4.5 , , FALSE , TRUE , , US 7 - 4.5 mm, 4.5\n2 , Knitting, knitting\n306 , Pullover , pullover , 319 , Sweater , sweater , 302 , Clothing , clothing , 301 , Categories, categories\n2 , 10 , 18 , 86 , 103 , 267 , 285 , 295 , female , adult , fitted , seamed , scoop-neck , written-pattern, worked-flat , other-edging\nTRUE , 4 , Pullover, pullover\n\n\n\n\n\n\n999215\n\n\n\n9992129\n\n\nOverall the path forward here is to collapse the columns instance by instance and then filter out redundant information while keeping some identification for potential merge.\n\nPattern_needle_sizes\nFor pattern_needle_size we can record both crochet and knit values and then clear out redundant information. In this process we somehow lost a couple data entries, I’ll look into this and just blast through the rest. I added a distinct because some patterns had multiple needles sets but I don’t really care so I’ll just take one of them at random.\n\n\nCode\n## Example:\npatterns_df_denest[5,24]\n\n## precleaning step:\ndim_accum &lt;- c()\nfor(i in 1:nrow(patterns_df_denest)){\n  each &lt;- patterns_df_denest[i,24][[1]] %&gt;% class()\n  dim_accum &lt;- c(dim_accum,each)\n}\npatterns_df_denest_prep &lt;- patterns_df_denest\n\npatterns_df_denest_prep$needle_check &lt;- dim_accum\n\n## Unnesting\npatterns_df_denest1 &lt;- patterns_df_denest_prep %&gt;% \n  filter(dim_accum == \"data.frame\") %&gt;%\n  unnest(pattern_needle_sizes, names_sep = \"_nested_\") %&gt;%\n  distinct(id,permalink,.keep_all = TRUE) %&gt;% select(-c(\n    \"pattern_needle_sizes_nested_id\",\"pattern_needle_sizes_nested_us_steel\",\"pattern_needle_sizes_nested_crochet\",\"pattern_needle_sizes_nested_knitting\",\"pattern_needle_sizes_nested_pretty_metric\",\"needle_check\" # removing the other redundant columns\n  ))\n\n\n\n    \n\nA data.frame: 3 × 9\n\n\n\nid\nus\nmetric\nus_steel\ncrochet\nknitting\nhook\nname\npretty_metric\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\n21\n1½\n2.5\n\nFALSE\nTRUE\n\nUS 1½ - 2.5 mm\n2.5\n\n\n2\n20\n2½\n3.0\n\nFALSE\nTRUE\n\nUS 2½ - 3.0 mm\n3\n\n\n3\n9\n9\n5.5\n\nFALSE\nTRUE\nI\nUS 9 - 5.5 mm\n5.5\n\n\n\n\n\n\n\n\n\n\nCraft\nThere is a similar process for craft which may also have the side effect of allowing us to remove some of the columns from the needles dataset.\n\n\nCode\n## Example\npatterns_df_denest1[3,28][[1]]\n\npatterns_df_denest2 &lt;- patterns_df_denest1 %&gt;% \n  unnest(craft, names_sep = \"_nested_\") %&gt;%\n  distinct(id,permalink,.keep_all = TRUE) %&gt;% select(-c(\n    \"craft_nested_id\",\"craft_nested_permalink\"\n  ))\n\n\n\n    \n\nA data.frame: 1 × 3\n\n\n\nid\nname\npermalink\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\n2\nKnitting\nknitting\n\n\n\n\n\n\n\n\n\n\nPattern_type\nThis is an important column to be able to separate out as well as this information will be acting as the main target column for all of the prediction datasets.\n\n\nCode\n## Example\npatterns_df_denest2[6,31][[1]]\n\npatterns_df_denest3 &lt;- patterns_df_denest2 %&gt;% \n  unnest(pattern_type,names_sep = \"_nested_\") %&gt;%\n  distinct(id,permalink,.keep_all = TRUE) %&gt;% select(-c(\n    \"pattern_type_nested_id\",\"pattern_type_nested_name\",\"pattern_type_nested_clothing\"\n  ))\n\n\n\n    \n\nA data.frame: 1 × 4\n\n\n\nclothing\nid\nname\npermalink\n\n\n\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\nTRUE\n4\nPullover\npullover\n\n\n\n\n\n\n\n\nWe were successful although the naming convention may be changed later on as the names of the columns are fairly dense.\n\n\nPattern_categories\nThe pattern categories may not have as much importance later on as there are far too many categories although there may be room for visualizations in the categories.\n\n\nCode\n## Example\npatterns_df_denest3[[30277,29]][[1]] %&gt;% class()\n\n#1:nrow(patterns_df_denest3)\n\nfor(i in 1:nrow(patterns_df_denest3)){\n    piece &lt;- patterns_df_denest3[[i,29]] \n    if(class(piece[[1]]) == \"data.frame\"){\n      piece &lt;- piece %&gt;%\n                  as.data.frame() %&gt;%\n                  select(permalink,parent.permalink)\n      piece &lt;- piece[1,]\n    }\n    else{\n      piece &lt;- c(NA,NA) %&gt;% t() %&gt;% as.data.frame()\n      colnames(piece) = c(\"permalink\",\"parent.permalink\")\n    }\n                   \n    if(i == 1){\n      category_accum &lt;- piece\n    } else{\n      category_accum &lt;- rbind(category_accum,piece)\n    }\n}\n\ncolnames(category_accum) &lt;- c(\"category_permalink\",\"category_parent_permalink\")\n\nnrow(category_accum)\nnrow(patterns_df_denest3)\n\n## Joining\npatterns_df_denest4 &lt;- cbind(patterns_df_denest3,category_accum) %&gt;%\n                          select(-pattern_categories)\n\n\n'list'\n\n\n96537\n\n\n96537\n\n\n\n\nPattern_attributes\nPattern attributes are another fairly difficult nested frame to consider as some patterns have multiple categories or categories are further nested. For this analysis I attempted it with the code below but have left it for future analysis.\n\n\nCode\n## Example:\npatterns_df_denest4[[1,29]]\n\n## Getting list of attributes\n\nattributes_total &lt;- c()\nfor(i in 1:nrow(patterns_df_denest4)){\n  attributes &lt;- patterns_df_denest4[[i,29]]$permalink\n  attributes_total &lt;- c(attributes_total,attributes) %&gt;% unique()\n}\n\nattributes_table &lt;- data.frame(t(rep(NA,length(attributes_total))))\ncolnames(attributes_table) &lt;- attributes_total[order(attributes_total)]\nattributes_table &lt;- attributes_table[-1,]\nattributes_table_og &lt;- attributes_table\n\nfor(i in 1:1){\n  print(i/21)\n  print(Sys.time())\n  attributes_table_i &lt;- attributes_table_og\n  for(g in 1:(nrow(patterns_df_denest4)/21)){\n    print(g)\n    attributes &lt;- patterns_df_denest4[[g,29]]$permalink\n    attributes_g &lt;- data.frame(t(rep(1,length(attributes))))\n    colnames(attributes_g) &lt;- attributes\n    attributes_table_i &lt;- rbind.fill(attributes_table_i,attributes_g)\n  }\n  rbind(attributes_table,attributes_table_i)\n}"
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#type-casting-and-column-renaming",
    "href": "data_cleaning/data_cleaning.html#type-casting-and-column-renaming",
    "title": "Data Cleaning",
    "section": "Type casting and column renaming",
    "text": "Type casting and column renaming\nNow that most of the nested dataframes have been unpacked, it’s time to check for column types. Some of the values are characters when I want them to be Integers so I will be going through it all and getting the desired values. The names are also horrendous given the multiple rounds of unnesting so I will go through and adjust for relevance, finally reordering the columns for ease of use in the future.\n\n\nCode\npatterns_denested &lt;- patterns_df_denest4\n\ncolnames(patterns_denested)\n\ncolnames(patterns_denested)[24:31] &lt;- c(\"needle_sizes_us\",\"needle_sizes_metric\",\"hook_size\",\"needle_sizes_name\",\"craft\",\"pattern_attributes\",\"pattern_type\",\"pattern_desc\")\n\n\nWith the columns as they are we will now be shifting some of the columns to numeric measurements. Most columns are already in character form so the categorical or word based groups are not as important to adapt. The exception to this will be the yarn_weight_decription column as there are many names for sizes but no specific ranking so I will be grouping some of them together.\n\n\nCode\nnumeric_columns &lt;- c(1,3,7,8,9,10,11,12,14,15,18,19,20,21,24)\n\nfor(i in 1:ncol(patterns_denested)){\n  if(i %in% numeric_columns){\n    patterns_denested[,i] &lt;- patterns_denested[,i] %&gt;% as.numeric()\n  }\n}\n\nlevels(factor(patterns_denested$yarn_weight_description))\nfor(i in 1:nrow(patterns_denested)){\n  value &lt;- patterns_denested[i,22]\n  if(value == \"Lace\"|value == \"Thread\"|value == \"Light Fingering\"){\n    patterns_denested[i,22] &lt;- 0\n  } else if(value == \"Fingering (14 wpi)\"|value == \"Cobweb\"){\n    patterns_denested[i,22] &lt;- 1\n  } else if(value == \"Sport (12 wpi)\"|value == \"DK / Sport\"){\n    patterns_denested[i,22] &lt;- 2\n  }  else if(value == \"DK (11 wpi)\"| value == \"\"| value == \"Any gauge - designed for any gauge\"){\n    patterns_denested[i,22] &lt;- 3\n  } else if(value == \"Aran / Worsted\"| value == \"Aran (8 wpi)\"|value == \"Worsted (9 wpi)\"){\n    patterns_denested[i,22] &lt;- 4\n  } else if(value == \"Bulky (7 wpi)\"|value == \"Super Bulky (5-6 wpi)\"){\n    patterns_denested[i,22] &lt;- 5\n  } else if(value == \"Jumbo (0-4 wpi)\"){\n    patterns_denested[i,22] &lt;- 6\n  }\n}\n\npatterns_denested$yarn_weight_description &lt;- patterns_denested$yarn_weight_description %&gt;% as.numeric()"
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#conclusion",
    "href": "data_cleaning/data_cleaning.html#conclusion",
    "title": "Data Cleaning",
    "section": "Conclusion",
    "text": "Conclusion\nThe data has now been prepared for use in models. There will be further trimming done around outliers but that will be left to the exploratory data analysis tab where more understanding of the underlying phenomena can be found. Otherwise we have suitable columns for both target and predictor variables, categories which can represent the distribution of patterns available, and relevant information about patterns which can be used to better understand the dataset we have.\nThe last step is to download the cleaned data set and begin analysis from there."
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#downloading-data-sets",
    "href": "data_cleaning/data_cleaning.html#downloading-data-sets",
    "title": "Data Cleaning",
    "section": "Downloading data sets",
    "text": "Downloading data sets\n\n\nCode\npatterns_df_denest_done &lt;- patterns_denested %&gt;% select(-pattern_attributes)\nwrite.csv(patterns_df_denest_done,\"../data/pattern_data_cleaned.csv\")"
  },
  {
    "objectID": "decision_trees/decision_trees.html",
    "href": "decision_trees/decision_trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Similar to the Naive Bayes tab, I will be using decision trees to predict the types of patterns based off of the given information such as amount of yards, size of the needles, and gauge. I was only able to reach a predictive capability of 47% or so with the Naive Bayes model so I’m hoping to improve upon the predictive capability and make it better than a coin toss."
  },
  {
    "objectID": "decision_trees/decision_trees.html#introduction",
    "href": "decision_trees/decision_trees.html#introduction",
    "title": "Decision Trees",
    "section": "",
    "text": "Similar to the Naive Bayes tab, I will be using decision trees to predict the types of patterns based off of the given information such as amount of yards, size of the needles, and gauge. I was only able to reach a predictive capability of 47% or so with the Naive Bayes model so I’m hoping to improve upon the predictive capability and make it better than a coin toss."
  },
  {
    "objectID": "decision_trees/decision_trees.html#methods",
    "href": "decision_trees/decision_trees.html#methods",
    "title": "Decision Trees",
    "section": "Methods",
    "text": "Methods\nDecision trees work by making splits in the data based off of a minimizing algorithm of the residual sum of squares. That’s a mouthful, but it keeps making these decisions until it is able to categorize the data based off of the hyperparameters given.\nDecision trees are also a weak algorithm which means that each time a tree is run there will be slight differences in its arrangement. This also allows it to be used for more advanced machine learning methods such as boosting and bagging. I will later take a boosting, or random forest, approach to improve the predictive capability of the model.\nWhen discussing the hyperparameters of the decision trees, the main changes you can make are adjustments to the maximum depth of the tree, the threshold for splitting the data, and what features to use in the first place. I will be running another feature selection test to see what works best for decision trees. I will be mostly leaving the threshold for splitting data to the default values but I will be testing an optimal depth for the trees.\nTo mention the boosting approach again, boosting works by accumulating trees with different splits and then taking the most likely results as the classifier. To be clear, these splits are not random and instead try to make up for the mistakes of the previous algorithms, this cumulative approach covers some of the weaknesses of decision trees to begin with.\nI look forward to seeing if this approach will be more impactful on the data and give better results.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom IPython.display import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nBelow I have some more information about the data which is similarly covered in previous tabs.\n\n\nCode\ndata = pd.read_csv(\"../data/pattern_data_updated.csv\")\ndata.shape\ndata.columns\n\n\nIndex(['Unnamed: 0', 'comments_count', 'currency', 'price', 'currency_symbol',\n       'free', 'published', 'difficulty_average', 'favorites_count',\n       'projects_count', 'queued_projects_count', 'gauge', 'gauge_divisor',\n       'gauge_pattern', 'row_gauge', 'id', 'name', 'permalink',\n       'rating_average', 'rating_count', 'yardage', 'yardage_max',\n       'yarn_weight_description', 'yarn_weight', 'needle_sizes_us',\n       'needle_sizes_metric', 'hook_size', 'needle_sizes_name', 'craft',\n       'pattern_type', 'pattern_desc', 'category_permalink',\n       'category_parent_permalink', 'price_adjusted'],\n      dtype='object')\n\n\n\n\nCode\ndf_info = pd.concat([data.dtypes,data.min(),data.mean(),data.max()],axis = 1)\ndf_info.rename(columns = {0:\"dtypes\",1:\"min\",2:\"mean\",3:\"max\"},inplace = True)\ndf_info\n\n\n\n\n\n\n\n\n\ndtypes\nmin\nmean\nmax\n\n\n\n\nUnnamed: 0\nint64\n1\n4.815500e+04\n96309\n\n\ncomments_count\nint64\n0\n6.735549e+00\n1509\n\n\ncurrency\nobject\nNaN\nNaN\nNaN\n\n\nprice\nfloat64\n0.0\n7.597368e+00\n1760.0\n\n\ncurrency_symbol\nobject\nNaN\nNaN\nNaN\n\n\nfree\nobject\nFalse\n3.130373e-01\nTrue\n\n\npublished\nobject\nNaN\nNaN\nNaN\n\n\ndifficulty_average\nfloat64\n0.0\n2.055180e+00\n8.75\n\n\nfavorites_count\nint64\n0\n1.375928e+03\n92747\n\n\nprojects_count\nint64\n0\n1.207596e+02\n39438\n\n\nqueued_projects_count\nint64\n0\n1.677255e+02\n13874\n\n\ngauge\nfloat64\n0.0\n1.964429e+01\n99.99\n\n\ngauge_divisor\nfloat64\n1.0\n3.730453e+00\n4.0\n\n\ngauge_pattern\nobject\nNaN\nNaN\nNaN\n\n\nrow_gauge\nfloat64\n0.0\n2.635220e+01\n99.99\n\n\nid\nfloat64\n10.0\n1.061431e+06\n7285876.0\n\n\nname\nobject\n\" Little Star \" shoulder bag\nNaN\n﹤3-on-my Sleeves\n\n\npermalink\nobject\n---mr-tumnuss-leaf-scarf\nNaN\nzyperngras\n\n\nrating_average\nfloat64\n0.0\n3.141943e+00\n5.0\n\n\nrating_count\nint64\n0\n3.526073e+01\n9516\n\n\nyardage\nfloat64\n0.0\n5.559181e+02\n5000.0\n\n\nyardage_max\nfloat64\n0.0\n8.351688e+02\n5000.0\n\n\nyarn_weight_description\nint64\n0\n2.677486e+00\n6\n\n\nyarn_weight\nfloat64\nNaN\nNaN\nNaN\n\n\nneedle_sizes_us\nfloat64\n0.0\n5.807764e+00\n50.0\n\n\nneedle_sizes_metric\nfloat64\n0.6\n4.117480e+00\n25.0\n\n\nhook_size\nobject\nNaN\nNaN\nNaN\n\n\nneedle_sizes_name\nobject\n0.6 mm\nNaN\nUS 9 - 5.5 mm\n\n\ncraft\nobject\nCrochet\nNaN\nMachine Knitting\n\n\npattern_type\nobject\nbaby\nNaN\nvest\n\n\npattern_desc\nobject\nNaN\nNaN\nNaN\n\n\ncategory_permalink\nobject\nNaN\nNaN\nNaN\n\n\ncategory_parent_permalink\nobject\nNaN\nNaN\nNaN\n\n\nprice_adjusted\nfloat64\n0.0\n3.950653e+00\n20.0"
  },
  {
    "objectID": "decision_trees/decision_trees.html#class-distribution",
    "href": "decision_trees/decision_trees.html#class-distribution",
    "title": "Decision Trees",
    "section": "Class Distribution",
    "text": "Class Distribution\nI will be doing a 80-20 train-test split for my data and using my features to predict the type of pattern, the extent of which is listed below. Having a target column with so many values is limiting the impact of the model itself but I believe that this application will have more impact than guessing whether or not a pattern was free for instance. There are also some pattern groups which are heavily over or underrepresented which could skew results to some extent. For instance there are very few jackets and tens of thousands of hats. I will see if this has an effect and see if this model type will be effective.\n\n\nCode\ndata['pattern_type'].value_counts()\n\n\nshawl         14261\npullover      12403\nhat           12205\nsocks         11989\ngloves         5641\ncardigan       5567\nchild          4960\ntoys           4564\nscarf          4159\nother          3526\nhome           3418\nbaby           3308\nblanket        2876\ntee            1671\ncamisole       1598\nbag            1134\ndishcloth      1047\nvest            790\npet             533\ndress-suit      194\nshrug           182\njacket          175\nskirt            93\nnaughty          15\nName: pattern_type, dtype: int64\n\n\n\n\nCode\nX = data[[\"yardage\",\"yardage_max\",\"gauge\",\"needle_sizes_us\"]]\nY = data[[\"pattern_type\"]]\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = .2,random_state = 1)\n\nprint(type(x_train), x_train.shape)\nprint(type(y_train), y_train.shape)\nprint(type(x_test), x_test.shape)\nprint(type(y_test), y_test.shape)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt; (77047, 4)\n&lt;class 'pandas.core.frame.DataFrame'&gt; (77047, 1)\n&lt;class 'pandas.core.frame.DataFrame'&gt; (19262, 4)\n&lt;class 'pandas.core.frame.DataFrame'&gt; (19262, 1)"
  },
  {
    "objectID": "decision_trees/decision_trees.html#baseline-model",
    "href": "decision_trees/decision_trees.html#baseline-model",
    "title": "Decision Trees",
    "section": "Baseline Model",
    "text": "Baseline Model\nI will be running a single decision tree with 4 classifers as a baseline model without any boosting or hyperparameter tuning in order to show the expected results.\n\n\nCode\nfrom sklearn import tree\nfrom sklearn.metrics import classification_report\n\nmodel = tree.DecisionTreeClassifier()\nmodel = model.fit(x_train,y_train)\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\n\n\nCode\n\nprint(classification_report(y_train,yp_train,output_dict=True)['accuracy'])\nprint(classification_report(y_test,yp_test,output_dict=True)['accuracy'])\n\n\n0.5263670227263878\n0.4508358425916312\n\n\nLooking at the accuracy values for the training and test sets we can see that this model has about equal performance to the Naive Bayes model in this state. The training set is only able to achieve a predictive capability of 52% percent which is incredibly low for a trained model but on par for this data. To follow this, there may be some overfitting to the training set as the test data is only 45 percent accurate. This could also be due to misfitting with the large number of variables but that drop between the two is fairly harrowing. I’ll now take some time to pick strong hyperparameters and boost this model."
  },
  {
    "objectID": "decision_trees/decision_trees.html#feature-selection",
    "href": "decision_trees/decision_trees.html#feature-selection",
    "title": "Decision Trees",
    "section": "Feature Selection",
    "text": "Feature Selection\nFor feature selection I’m planning on doing a test for maximum depth and also for feature selection. I noticed feature selection changed alot model by model so what was needed for decision trees might be different than the optimal ones for the naive bayes models. I will print out the best subset for reference.\n\n\nCode\ndef train_DT_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = .2,random_state = 1)\n\n    # INITIALIZE MODEL \n    model = tree.DecisionTreeClassifier()\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test)\n\n    return (acc_train,acc_test)\n\n\n\n\nCode\nx = data[[\"price\",\n        \"free\",\n        \"difficulty_average\",\n        \"gauge\",\"gauge_divisor\",\"row_gauge\",\n        \"rating_average\",\"rating_count\",\n        \"yardage\",\"yardage_max\",\n        \"yarn_weight_description\",\n        \"needle_sizes_metric\",\n        \"pattern_type\"]].dropna()\ny = x[\"pattern_type\"]\nx.drop(\"pattern_type\",inplace = True,axis = 1)\nx[\"free\"] = x[\"free\"].astype(\"category\")\n\nimport itertools\n\nsubsets = []\nlist1 = [*range(1,x.shape[1])]; #print(list1)\n\ntrain_accs_g = []\ntest_accs_g = []\nnum_features_g = []\nbest_test_acc_g = 0\n\n# x.shape[1]+1\nfor l in range(1,x.shape[1]+1):\n    for subset in itertools.combinations(x.columns, l):\n        train_acc, test_acc = train_DT_model(x.loc[:,list(subset)],y)\n        train_accs_g.append(train_acc)\n        test_accs_g.append(test_acc)\n        num_features_g.append(len(list(subset)))\n        if test_acc &gt; best_test_acc_g:\n            best_test_acc_g = test_acc\n            best_subset_g = list(subset)\n\nprint(best_subset_g)\n\n\n['price', 'gauge', 'gauge_divisor', 'row_gauge', 'rating_count', 'yardage', 'yardage_max', 'yarn_weight_description', 'needle_sizes_metric']\n\n\nGiven this optimal set, I will be checking for the accuracy across many different maximum ranges to see if there is a point where the tree becomes overfitted to the data. Lowering the max depth will also help speed up the boosting process.\n\n\nCode\ntest_results=[]\ntrain_results=[]\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(data[best_subset_g],Y,test_size = .2,random_state = 1)\n\nfor num_layer in range(1,30):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train,y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    test_results.append([num_layer,accuracy_score(y_test, yp_test)])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train)])\n\ntest_df = pd.DataFrame(test_results)\ntrain_df = pd.DataFrame(train_results)\n\nplt.plot(train_df.iloc[:,0],train_df.iloc[:,1],\"o-b\")\nplt.plot(test_df.iloc[:,0],test_df.iloc[:,1],\"o-r\")\n\nplt.xlabel(\"Number of Layers in decision tree\")\nplt.ylabel(\"ACCURACY (Y=0): Training (blue) and Test (red)\")\n\n\nText(0, 0.5, 'ACCURACY (Y=0): Training (blue) and Test (red)')\n\n\n\n\n\nFrom looking at the graph, there are deviations in the accuracy of the training and testing data around 7 layers and widens a bit until splitting upward and downwards after 8. This implies that 8 as a depth works best in order to maximize the accuracy of the testing data set and avoid overfitting. We can see that as the maximum depth increases the training accuracy increases concurrently but after a certain point the testing accuracy starts decreasing, this is a pretty good indicator that there is overfitting and is a pretty good visualization of its perils. So, going forward I will be using a max depth of 8 layers. I will quickly rerun the base model to show the benefits.\n\n\nCode\nfrom sklearn import tree\nfrom sklearn.metrics import classification_report\n\nmodel = tree.DecisionTreeClassifier(max_depth = 8)\nmodel = model.fit(x_train,y_train)\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\nprint(classification_report(y_train,yp_train,output_dict=True)['accuracy'])\nprint(classification_report(y_test,yp_test,output_dict=True)['accuracy'])\n\n\n0.47336041636922915\n0.46537223548956497\n\n\nThe testing accuracy rose to 46% while the testing accuracy remained similar. This is fine though and a definite improvement on the other data set, especially with the test and training accuracies being more similar. I will no go forward with the Random Forest method to try to improve this value and increase the accuracy of both of the datasets."
  },
  {
    "objectID": "decision_trees/decision_trees.html#random-forest-trial",
    "href": "decision_trees/decision_trees.html#random-forest-trial",
    "title": "Decision Trees",
    "section": "Random Forest Trial",
    "text": "Random Forest Trial\nI’m going to jump right into the Random Forest. I’ll start by testing hyperparameters out of curiosity then I’m going to use that value to get the best prediction for the test data. I also found around 100 trees to be sufficient with more than that giving similar results for longer runtimes.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\n\nX = data[best_subset_g]\nX[\"pattern_type\"] = data[\"pattern_type\"]\n\nX = X.dropna()\n\nY = X[[\"pattern_type\"]]\nX = X.drop(\"pattern_type\",axis = 1)\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = .2,random_state = 1)\n\ntest_results=[]\ntrain_results=[]\n\nfor i in range(1,25):\n    num_layer = i\n    clf = RandomForestClassifier(max_depth = i, n_estimators= 100, random_state=0)\n    clf.fit(x_train,y_train.to_numpy())\n    yp_train = clf.predict(x_train)\n    yp_test = clf.predict(x_test)\n\n    test_results.append([num_layer,accuracy_score(y_test, yp_test)])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train)])\n\ntest_df = pd.DataFrame(test_results)\ntrain_df = pd.DataFrame(train_results)\n\nplt.plot(train_df.iloc[:,0],train_df.iloc[:,1],\"o-b\")\nplt.plot(test_df.iloc[:,0],test_df.iloc[:,1],\"o-r\")\n\nplt.xlabel(\"Number of Layers in decision tree\")\nplt.ylabel(\"ACCURACY (Y=0): Training (blue) and Test (red)\")\n\n\n\nText(0, 0.5, 'ACCURACY (Y=0): Training (blue) and Test (red)')\n\n\n\n\n\nSo it is looking like roughly 15 as a max depth is best for test accuracy in the random forest, there are marginal increases but I’m valuing closeness between the test and training models. I think I will use a dept of around 12. The difference between test and train might be different but I’m hoping it will improve the test overall. Now to see the accuracy at that amount I will run a final random forest.\n\n\nCode\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = .2,random_state = 1)\n\nclf = RandomForestClassifier(max_depth = 12, n_estimators= 500, random_state=0)\nclf.fit(x_train,y_train)\nyp_train = clf.predict(x_train)\nyp_test = clf.predict(x_test)\n\nprint(classification_report(y_train,yp_train,output_dict=True)['accuracy'])\nprint(classification_report(y_test,yp_test,output_dict=True)['accuracy'])\n\n\n0.6482692162346005\n0.5541531252361878\n\n\nAlright, there was some improvement with 55 percent accuracy which is better than the decision tree but not quite as high as I would hope overall. Anyhow, this isn’t the worst but it is a bit disappointing showing for random forests in this case, only improving the model by 10% accuracy."
  },
  {
    "objectID": "decision_trees/decision_trees.html#final-results",
    "href": "decision_trees/decision_trees.html#final-results",
    "title": "Decision Trees",
    "section": "Final results",
    "text": "Final results\nSo after fitting the model, hyperparameter tuning, and boosting. We managed to up the baseline of 46% accuracy to 55% which does meet a coinflip criteria but isn’t as good as I would have hoped. This is pretty low but I think the best thing to do is recheck the data to increase the amount of rows and the accuracy of the data. There seems to be a pretty hard cap here which cannot be breached without more and better data. Overfitting clearly becomes a problem after 50% or so that may be something to watch out for."
  },
  {
    "objectID": "decision_trees/decision_trees.html#conclusions",
    "href": "decision_trees/decision_trees.html#conclusions",
    "title": "Decision Trees",
    "section": "Conclusions",
    "text": "Conclusions\nDecision trees are a very interesting classifying method which take a branching and hierarchical approach in comparison to the other more optimization focused models. I personally made it my task to use classifying random forests to try to predict the type of a pattern based on some sample pattern information and was succesful at improving upon previous methods.\nI was marginally successful in my analysis with a slightly better than coin flip success rate. This means that given the information about a knitting pattern there is a 55% chance that the algorithm guesses the type of pattern correctly. This wouldn’t fly in a market setting as there is still way too much variability but I still consider it a success as it would be hard to even reach half that success rate just using human intuition. Random forests were also able to improve upon the previous record which shows the value of the model in the larger context."
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "eda/eda.html#goals-for-eda",
    "href": "eda/eda.html#goals-for-eda",
    "title": "Exploratory Data Analysis",
    "section": "Goals for EDA",
    "text": "Goals for EDA\nIn the process of completing this analysis I hope to be able to find pre-existing patterns in the data, visualize interesting relationships, and raise questions which will be answered by future analytic tabs. I am lucky that I already have a fair understanding of the values in the columns due to my experience with the database they are from as well as from knitting with many of these patterns."
  },
  {
    "objectID": "eda/eda.html#importing-data-and-overview",
    "href": "eda/eda.html#importing-data-and-overview",
    "title": "Exploratory Data Analysis",
    "section": "Importing data and overview",
    "text": "Importing data and overview\nThe first step is to import the cleaned data from the previous data cleaning section. I will also be including the column names as a reminder for context.\n\n\nCode\npattern_df &lt;- read.csv(\"../data/pattern_data_cleaned.csv\")\npattern_df &lt;- pattern_df[,-1]\ncolnames(pattern_df)\n\n\n\n'comments_count''currency''price''currency_symbol''free''published''difficulty_average''favorites_count''projects_count''queued_projects_count''gauge''gauge_divisor''gauge_pattern''row_gauge''id''name''permalink''rating_average''rating_count''yardage''yardage_max''yarn_weight_description''yarn_weight''needle_sizes_us''needle_sizes_metric''hook_size''needle_sizes_name''craft''pattern_type''pattern_desc''category_permalink''category_parent_permalink'\n\n\n\nSubgroups of Data\nThe data can be broken up into a few sections dependent on the variables themselves:\nPopularity measures: this group includes the count of comments, favorites, projects, queued projects, and the ratings count and average. These measures will naturally be higher when a pattern was seen and used by more of the users of the site.\nCost measures: this group includes the price, its currency, and whether or not the pattern is free in the first place.\nPattern specific measures: this is a group consisting of many of the factors such as yardage, gauge, and needle or hook size which show for up on most pattern descriptions and give context for the user to prepare for the pattern.\nClassifying factors: this includes the pattern type, craft, attributes, and category. These values give context about what the end goal of the pattern will be. This also includes name and id which act more as identifying information.\nMisc factors: there are some variables remaining such as the text data and the difficulty which act as auxiliary to the data and will be considered if interesting.\n\n\nCorrelation\nI would also like to do a quick correlation check on the variables I have. Unfortunately I have too many variables, so I was forced to narrow my selection down to seven of note.\n\n\nCode\nnumeric_data &lt;- select_if(pattern_df, is.numeric)\n\nnumeric_slice &lt;- numeric_data %&gt;% select(c(price, difficulty_average,projects_count,rating_average,yardage,needle_sizes_us,yarn_weight_description))\n# data was too large so I'm taking a sample to visualize\n\npairs(numeric_slice)\n\n\n\n\n\n\n\nData Summaries\nI’d also like to do a quick summary of all of the data available. It is dense in amount but gives a good overview that will start the rest of my analysis.\n\n\nCode\nsummary(pattern_df)\n\n\n comments_count      currency             price         currency_symbol   \n Min.   :  -2.00   Length:96537       Min.   :   0.00   Length:96537      \n 1st Qu.:   0.00   Class :character   1st Qu.:   5.00   Class :character  \n Median :   2.00   Mode  :character   Median :   6.00   Mode  :character  \n Mean   :   6.73                      Mean   :  11.53                     \n 3rd Qu.:   6.00                      3rd Qu.:   7.90                     \n Max.   :1509.00                      Max.   :1760.00                     \n                                      NA's   :32599                       \n    free          published         difficulty_average favorites_count\n Mode :logical   Length:96537       Min.   :0.000      Min.   :    0  \n FALSE:66294     Class :character   1st Qu.:0.000      1st Qu.:  138  \n TRUE :30242     Mode  :character   Median :2.300      Median :  443  \n NA's :1                            Mean   :2.056      Mean   : 1376  \n                                    3rd Qu.:3.333      3rd Qu.: 1343  \n                                    Max.   :8.750      Max.   :92747  \n                                                                      \n projects_count    queued_projects_count     gauge       gauge_divisor  \n Min.   :    0.0   Min.   :    0.0       Min.   : 0.00   Min.   :1.000  \n 1st Qu.:    7.0   1st Qu.:   12.0       1st Qu.:15.00   1st Qu.:4.000  \n Median :   20.0   Median :   41.0       Median :20.00   Median :4.000  \n Mean   :  120.8   Mean   :  167.7       Mean   :19.64   Mean   :3.731  \n 3rd Qu.:   69.0   3rd Qu.:  138.0       3rd Qu.:24.00   3rd Qu.:4.000  \n Max.   :39438.0   Max.   :13874.0       Max.   :99.99   Max.   :4.000  \n                                         NA's   :15954   NA's   :2803   \n gauge_pattern        row_gauge           id              name          \n Length:96537       Min.   : 0.00   Min.   :     10   Length:96537      \n Class :character   1st Qu.:19.00   1st Qu.: 814395   Class :character  \n Mode  :character   Median :28.00   Median :1125383   Mode  :character  \n                    Mean   :26.35   Mean   :1060859                     \n                    3rd Qu.:34.00   3rd Qu.:1270657                     \n                    Max.   :99.99   Max.   :7285876                     \n                    NA's   :25902                                       \n  permalink         rating_average   rating_count        yardage       \n Length:96537       Min.   :0.000   Min.   :   3.00   Min.   :    0.0  \n Class :character   1st Qu.:0.000   1st Qu.:   5.00   1st Qu.:  175.0  \n Mode  :character   Median :4.580   Median :  12.00   Median :  365.0  \n                    Mean   :3.143   Mean   :  52.88   Mean   :  567.9  \n                    3rd Qu.:4.846   3rd Qu.:  36.00   3rd Qu.:  831.0  \n                    Max.   :5.000   Max.   :9516.00   Max.   :68362.0  \n                                    NA's   :32159     NA's   :12084    \n  yardage_max        yarn_weight_description yarn_weight    needle_sizes_us \n Min.   :      0.0   Min.   :0.000           Mode:logical   Min.   : 0.000  \n 1st Qu.:    225.0   1st Qu.:1.000           NA's:96537     1st Qu.: 4.000  \n Median :    465.0   Median :3.000                          Median : 6.000  \n Mean   :    931.1   Mean   :2.677                          Mean   : 5.808  \n 3rd Qu.:   1378.0   3rd Qu.:4.000                          3rd Qu.: 8.000  \n Max.   :1093613.0   Max.   :6.000                          Max.   :50.000  \n NA's   :21985                                              NA's   :12887   \n needle_sizes_metric  hook_size         needle_sizes_name     craft          \n Min.   : 0.600      Length:96537       Length:96537       Length:96537      \n 1st Qu.: 3.000      Class :character   Class :character   Class :character  \n Median : 4.000      Mode  :character   Mode  :character   Mode  :character  \n Mean   : 4.117                                                              \n 3rd Qu.: 4.500                                                              \n Max.   :25.000                                                              \n                                                                             \n pattern_type       pattern_desc       category_permalink\n Length:96537       Length:96537       Length:96537      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n category_parent_permalink\n Length:96537             \n Class :character         \n Mode  :character         \n                          \n                          \n                          \n                          \n\n\nNow this is a lot of data. I’ll highlight a few trends that will not be impressed upon later. First off, the majority of the patterns are not free, that’s interesting to know as I only rarely ever pay for patterns. Someone also paid 1760 somethings for a pattern, I need to find the currency on that. I also need to find the pattern with the higher difficulty rating of 8.75. Some other fun facts are that some patterns are really popular with almost 100,000 interactions, there’s patterns that require hundreds of thousands of yards of yarn, even up to a million, when 931 is the average, and that needles go to size 25 which is 25 milliliter needles (those are larger than your thumb).\n\n\nInitial Data Visualizations\nNext, I ran a series of visualizations for my own use on the distribution of the numeric patterns. I’ll give a brief summary of what was found below and then go into more detail in the outlier section, providing more graphs and interpretations.\n\n\nCode\nnumeric_data &lt;- select_if(pattern_df, is.numeric)\n\nfor(i in 1:ncol(numeric_data)){\n  plot &lt;- ggplot(data = numeric_data, aes(x = numeric_data[,i])) +\n    geom_density(fill = \"#003DA5\",color = \"grey8\") +\n    xlab(colnames(numeric_data)[i]) +\n    theme_classic()\n  print(plot)\n}\n\n\nFrom this suite of visualizations I saw a lot of the individual variable behavior and some vaguely normal distributions, this could lean towards mean analysis or other determinations of distribution.\nThere are some common trends such as many 0 values and a more rounded distribution in the center or alternatively a large number at 0 and then a steep slope off with a long tail. The dataset contains many outliers as some patterns are incredibly popular, expensive, or require unique materials.\nA few standouts are the difficulty and ratings average with a real distribution after a spike at 0, needle sizes with a spiky normal like distribution, and gauge with interesting constructions for all its variation.\nOverall the distribution trends do not seem to be constant and there is much room to trim up outliers. After looking throught the categorical data that will be my next step. I do not go over these categories in the outlier section so I will leave the graphs here as a refernce point.\n\n\nCode\ncategorical_data &lt;- select_if(pattern_df, is.character)\ncategorical_data &lt;- categorical_data %&gt;%\n  select(-c(pattern_desc,published,gauge_pattern,name,permalink,category_permalink,needle_sizes_name))\n\nfor(i in 1:ncol(categorical_data)){\n  plot &lt;- ggplot(data = categorical_data, aes(y = categorical_data[,i], fill = \"blue\")) +\n    geom_bar() +\n    ylab(colnames(categorical_data)[i]) +\n    theme_classic()\n  print(plot)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the categorical data side: american currency seems to be the leading one used on the site, G hook was the most used hook size while knitting was more popular as a craft, and shawl was the most popular type of pattern and sweater the most popular category. There was less outlier bias as more of the variables seemed filled in."
  },
  {
    "objectID": "eda/eda.html#outlier-detection-and-removal",
    "href": "eda/eda.html#outlier-detection-and-removal",
    "title": "Exploratory Data Analysis",
    "section": "Outlier Detection and Removal",
    "text": "Outlier Detection and Removal\n\n\nCode\n## listing all numeric columns\ncolnames(numeric_data)\ncolnames(categorical_data)\n\n\n\n'comments_count''price''difficulty_average''favorites_count''projects_count''queued_projects_count''gauge''gauge_divisor''row_gauge''id''rating_average''rating_count''yardage''yardage_max''yarn_weight_description''needle_sizes_us''needle_sizes_metric'\n\n\n\n'currency''currency_symbol''hook_size''craft''pattern_type''category_parent_permalink'\n\n\n\nComments_count\nThe comments count has one notable outlier at 1509 comments with the next lowest pattern being at 619 comments. Fun bit of history here, this hat with all the comments is called the PussyHat project. It was knit by many for the women’s march of 2016 in response to Donald Trump being elected. It’s one of the most well known symbols of the women’s movement of that era with this being the place that most people seemed to have gotten the pattern from. I’m thinking I’ll keep this data point around as a known outlier; the rest form a decent sloping distribution so I think I’m okay leaving this as it.\nAlso there are no missing values in these rows so I won’t worry about that here.\n\n\nCode\n## Highest comment values\ncheck &lt;- pattern_df$comments_count %&gt;% table() %&gt;% as.data.frame() \ncheck[order(check$.),] %&gt;% tail(n = 10)\n\n## the pussy hat project\npattern_df %&gt;% \n  filter(comments_count == 1509) %&gt;% \n  select(comments_count,name,rating_average, projects_count,favorites_count)\n\npattern_df &lt;- pattern_df %&gt;% filter(comments_count &gt;= 0) \n\n\n\nA data.frame: 10 × 2\n\n\n\n.\nFreq\n\n\n\n&lt;fct&gt;\n&lt;int&gt;\n\n\n\n\n248\n387\n1\n\n\n249\n400\n1\n\n\n250\n406\n1\n\n\n251\n422\n1\n\n\n252\n442\n1\n\n\n253\n480\n1\n\n\n254\n531\n1\n\n\n255\n554\n1\n\n\n256\n619\n1\n\n\n257\n1509\n1\n\n\n\n\n\n\nA data.frame: 1 × 5\n\n\ncomments_count\nname\nrating_average\nprojects_count\nfavorites_count\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1509\nPussyHat Project\n4.63281\n12925\n13951\n\n\n\n\n\n\n\nPrice\nFor price there are some high values I may trim off as well as missing values that could easily be replaced with 0. On that latter point, if a value is missing then it most likely means the pattern is free and thus could be seen to cost 0 dollars.\nI also run into an issue here that patterns are listed in different currencies so thousands of Yuen for instance could represent a much smaller actual amount, I’ll see if there are any trends related to this.\n\n\nCode\n## relating price to currency\nprice_model &lt;- lm(data = pattern_df, price ~ currency)\nsummary(price_model)\n\n##\npattern_df$price %&gt;% table() %&gt;% tail()\n\npattern_df %&gt;% filter(price == 1760|\n                        price == 1500|\n                        price == 1300|\n                        price == 1296|\n                        price == 1000) %&gt;%\n                    select(price,name,rating_average)\n\n## setting NAs to 0\npattern_df[is.na(pattern_df$price),3] &lt;- 0\n\n\n\nCall:\nlm(formula = price ~ currency, data = pattern_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-433.33   -1.15   -0.10    1.12 1161.71 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7.5715     2.3235   3.259  0.00112 ** \ncurrencyAUD    0.1687     2.3689   0.071  0.94323    \ncurrencyBRL   24.5603     3.6738   6.685 2.32e-11 ***\ncurrencyCAD   -0.3000     2.3335  -0.129  0.89772    \ncurrencyCHF   -1.6255     2.4274  -0.670  0.50307    \ncurrencyCZK  123.4285     4.8181  25.618  &lt; 2e-16 ***\ncurrencyDKK   38.4054     2.3472  16.362  &lt; 2e-16 ***\ncurrencyEUR   -1.7659     2.3266  -0.759  0.44786    \ncurrencyGBP   -2.6926     2.3306  -1.155  0.24798    \ncurrencyHKD   48.6285     5.0193   9.688  &lt; 2e-16 ***\ncurrencyHRK    0.4185    13.5482   0.031  0.97536    \ncurrencyHUF 1025.7618     8.0488 127.442  &lt; 2e-16 ***\ncurrencyILS   19.1208     4.3707   4.375 1.22e-05 ***\ncurrencyINR  262.4785     6.4054  40.977  &lt; 2e-16 ***\ncurrencyJPY  590.7226     2.4794 238.257  &lt; 2e-16 ***\ncurrencyMXN   75.7618     8.0488   9.413  &lt; 2e-16 ***\ncurrencyNOK   50.2231     2.3500  21.371  &lt; 2e-16 ***\ncurrencyNZD    2.2708     2.4744   0.918  0.35877    \ncurrencyPLN   17.4911     2.5331   6.905 5.07e-12 ***\ncurrencyRUB  422.4285     4.8181  87.675  &lt; 2e-16 ***\ncurrencySEK   51.7658     2.3609  21.926  &lt; 2e-16 ***\ncurrencySGD    3.0560     3.1389   0.974  0.33026    \ncurrencyTWD  206.4285     4.8181  42.844  &lt; 2e-16 ***\ncurrencyUSD   -1.4691     2.3245  -0.632  0.52738    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.35 on 63913 degrees of freedom\n  (32598 observations deleted due to missingness)\nMultiple R-squared:  0.8949,    Adjusted R-squared:  0.8948 \nF-statistic: 2.365e+04 on 23 and 63913 DF,  p-value: &lt; 2.2e-16\n\n\n.\n 900 1000 1296 1300 1500 1760 \n   4    1    1    1    1    2 \n\n\n\nA data.frame: 6 × 3\n\n\nprice\nname\nrating_average\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\n1300\nVery Berry Garden\n4.913043\n\n\n1760\nSWEATER\n4.500000\n\n\n1000\nSzatyor csúszda\n0.000000\n\n\n1760\nRoad vest\n5.000000\n\n\n1296\nSpiral Socks No.21 ひし形\n4.000000\n\n\n1500\nDrazsé\n0.000000\n\n\n\n\n\nThe model says there is difference by price. I’ll take this as a chance to source some currency conversion rates and try to get all of the prices in standard US dollars.\n\n\nCode\ntransition &lt;- read.csv(\"../data/currency_transition.csv\", header = FALSE)\nexchange_rates &lt;- read.csv(\"../data/Currency_rate_11 10 2023.csv\")\n\n#head(transition)\n#head(exchange_rates)\n\ncolnames(transition) &lt;- c(\"country\",\"name\",\"currency\",\"something_idk\")\ncolnames(exchange_rates) &lt;- c(\"name\",\"dollar_to\",\"dollar_from\")\n#dim(transition)\n#dim(exchange_rates)\n\nexchange &lt;- left_join(x=transition,y=exchange_rates) %&gt;% select(c(name,currency,dollar_from))\n\nfiltered_exchange &lt;- exchange %&gt;% filter(currency %in% pattern_df$currency)\n\n#head(filtered_exchange)\n\ndistinct_exchange &lt;- filtered_exchange %&gt;% select(currency, dollar_from) %&gt;% distinct()\n\ndistinct_exchange &lt;- distinct_exchange[-8,]\n#head(distinct_exchange)\n\ndistinct_exchange[is.na(distinct_exchange$dollar_from) == TRUE,2] &lt;- 1\n\nprice_attempt &lt;- pattern_df %&gt;% select(c(price,currency,name))\ninterstice &lt;- left_join(price_attempt,distinct_exchange)\n\ninterstice &lt;- interstice %&gt;% mutate(price_adjusted = price * dollar_from)\n\n\nJoining with `by = join_by(name)`\nJoining with `by = join_by(currency)`\n\n\nAfter merging together all the relevant information, there is a decent estimate on price of everything in dollars.\n\n\nCode\ninterstice_na &lt;- interstice %&gt;% na.omit()\ninterstice_na[order(interstice_na$price_adjusted),] %&gt;% tail(10)\n\nggplot(data = interstice,aes(x = price_adjusted)) + geom_density() + theme_classic()\n\n\n\nA data.frame: 10 × 5\n\n\n\nprice\ncurrency\nname\ndollar_from\nprice_adjusted\n\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n58224\n100\nUSD\nBlooming Hydrangea Shawl\n1.000000\n100.0000\n\n\n72642\n100\nUSD\nSeasonal Sock Club\n1.000000\n100.0000\n\n\n14183\n213\nEUR\nHálfmáni\n1.067657\n227.4109\n\n\n26583\n213\nEUR\nHlýir\n1.067657\n227.4109\n\n\n55605\n213\nEUR\nKraginn\n1.067657\n227.4109\n\n\n48452\n216\nEUR\nVífilsfell\n1.067657\n230.6139\n\n\n85632\n220\nEUR\nCombinaison à torsades - M2838\n1.067657\n234.8845\n\n\n85633\n220\nEUR\nPull lion - M2858\n1.067657\n234.8845\n\n\n41918\n295\nGBP\nMaud\n1.221520\n360.3484\n\n\n90884\n295\nGBP\nWinter Warmers\n1.221520\n360.3484\n\n\n\n\n\nWarning message:\n\"Removed 32600 rows containing non-finite values (`stat_density()`).\"\n\n\n\n\n\nSo cannonically, the most expensive garments on the platform, in order, are from an Icelandic producer named Helene Magnanimous, a brand named Bergere de france, and, the queen of them all, a british designer Susan Crawford who priced a the pattern for a pair of mittens at OVER 300 DOLLARS!\nAnyway, the price being adjusted by currency actually gives a more standard measure which I will be adding to the overall data. Also below the three I mentioned earlier it’s just one off expensive patterns from random designers, not the top three like were listed above. The majority of patterns were between 0 and 20 with only 84 patterns being more expensive than that.\n\n\nCode\npattern_df$price_adjusted &lt;- interstice$price_adjusted\npattern_df &lt;- pattern_df %&gt;% filter(price_adjusted &lt;= 20)\n\nggplot(data = pattern_df,aes(x = price_adjusted)) + geom_density() +\n  theme_classic()\n\n\n\n\n\n\n\nDifficulty_average\nI don’t foresee there being an issue with this value, it seems to be well bound with an interesting distribution aside from all the 0s. This value needs to be assigned by users so if no one is using the pattern it won’t arise.\n\n\nFavorites_count\nFavorites count is similar to comments count with mostly low data but some outliers, similarly too there are no missing values as the database assigns the value.\n\n\nCode\n## Highest favorites count\ncheck &lt;- pattern_df$favorites_count %&gt;% table() %&gt;% as.data.frame() \ntop_10 &lt;- check[order(check$.),] %&gt;% tail(n = 10)\n\nas.numeric(as.character((top_10[,1])))\n\n## checking names \ntop_favorites &lt;- pattern_df %&gt;% \n  filter(favorites_count %in% as.numeric(as.character((top_10[,1])))) %&gt;%\n  select(name, favorites_count)\n\ntop_favorites[order(top_favorites$favorites_count,decreasing = TRUE),]\n\n\n\n70254718447186476446775538128682482825628795692747\n\n\n\nA data.frame: 10 × 2\n\n\n\nname\nfavorites_count\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n3\nNo Frills Sweater\n92747\n\n\n2\nThe Weekender\n87956\n\n\n8\nReyna\n82562\n\n\n4\nRanunculus\n82482\n\n\n6\nHermione's Everyday Socks\n81286\n\n\n5\nNightshift\n77553\n\n\n1\nFlax\n76446\n\n\n10\nHoney Cowl\n71864\n\n\n7\nBaa-ble Hat\n71844\n\n\n9\nThe Age of Brass and Steam Kerchief\n70254\n\n\n\n\n\nHmmm, so the patterns that come out of this are actually really cool and well made so I’m inclined to keep them, especially as there isn’t one or a few that are much larger. Mainly the distribution seems to have a long tail rather than there being specific outliers. The top pattern is also just a simple sweater which is interesting.\nI’m actually going to keep track of this list for my personal use as others seems to like these patterns as well.\n\n\nProjects_count and Queued_projects_count\nSo these two values are similar and I’m interested to find out which is larger on average and to understand some differences and extremes.\n\n\nCode\n#Highest project count\ncheck &lt;- pattern_df$projects_count %&gt;% table() %&gt;% as.data.frame() \ntop_10 &lt;- check[order(check$.),] %&gt;% tail(n = 10)\n\ntop_projects &lt;- pattern_df %&gt;% \n  filter(projects_count %in% as.numeric(as.character((top_10[,1])))) %&gt;%\n  select(name, projects_count)\ntop_projects[order(top_projects$projects_count,decreasing = TRUE),]\n\n# Highest queued project count\ncheck &lt;- pattern_df$queued_projects_count %&gt;% table() %&gt;% as.data.frame() \ntop_10 &lt;- check[order(check$.),] %&gt;% tail(n = 10)\n\ntop_queued_projects &lt;- pattern_df %&gt;% \n  filter(queued_projects_count %in% as.numeric(as.character((top_10[,1])))) %&gt;%\n  select(name, queued_projects_count)\ntop_queued_projects[order(top_queued_projects$queued_projects_count,decreasing = TRUE),]\n\nprint(\"Mean project count\")\nmean(pattern_df$projects_count)\nprint(\"mean queued project count\")\nmean(pattern_df$queued_projects_count)\n\n\n\nA data.frame: 10 × 2\n\n\n\nname\nprojects_count\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n3\nHermione's Everyday Socks\n39438\n\n\n7\nHitchhiker\n35602\n\n\n4\nSockhead Slouch Hat\n28540\n\n\n6\nHoney Cowl\n27999\n\n\n5\nBarley\n27621\n\n\n1\nFlax\n27442\n\n\n9\nClapotis\n23663\n\n\n8\nMonkey Socks\n23117\n\n\n10\nFetching\n21357\n\n\n2\nRanunculus\n20950\n\n\n\n\n\n\nA data.frame: 10 × 2\n\n\n\nname\nqueued_projects_count\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n6\nHoney Cowl\n13874\n\n\n8\nGAP-tastic Cowl\n13570\n\n\n4\nHermione's Everyday Socks\n13359\n\n\n9\nStar Crossed Slouchy Beret\n12973\n\n\n2\nThe Weekender\n12443\n\n\n5\nReyna\n12360\n\n\n10\nFebruary Lady Sweater\n11449\n\n\n7\nowls\n11385\n\n\n3\nRanunculus\n11376\n\n\n1\nFlax\n11367\n\n\n\n\n\n[1] \"Mean project count\"\n[1] \"mean queued project count\"\n\n\n120.790940261247\n\n\n167.699089468287\n\n\nAgain these are interesting results, no na values, and there are some familiar patterns to look at. I’m most interested to see Ranunculus showing up once or twice. Similarly, it seems that the projects count has a lower average but higher extremes than the queued projects. This suggests more people have done specific patterns but as a whole people are waiting to do more projects than they have completed. Knowing knitting people, this makes sense.\n\n\nCode\nggplot(data = pattern_df) + \n  geom_density(aes(x = projects_count), color = \"grey6\") + \n  geom_density(aes(x = queued_projects_count),color = \"blue\") +\n  xlim(0,250) +\n  theme_classic()+\n  labs(title = \"Project count is gray, queued project count in blue\")\n\n\nWarning message:\n\"Removed 8652 rows containing non-finite values (`stat_density()`).\"\nWarning message:\n\"Removed 14643 rows containing non-finite values (`stat_density()`).\"\n\n\n\n\n\nThis graph of the data shows the effect on the other side of the equation with similarly more patterns with a higher project count to start and the queued projects being slightly more as the numbers increase.\nWith these values and the ratings average and count later on I’m thinking of putting together a popularity metric and comparing it to the Ravelry specific one. Speaking of which, I’ll jump ahead and look at ratings.\n\n\nRating_average and Rating_count\nRatings in the database are scores that the users have given the patterns they like or dislike. The average is heavily skewed by the number of people rating it so there are many patterns with 5 star ratings from only a handful of people but some with mid 4.5s and thousands. Finding which pattern is rated the highest would require some thresholding which I may enter into later. Mostly at this point I want to do a quick test of which values have been rated the most and replace the NA’s in rating count with 0s for future use.\n\n\nCode\nggplot(data = pattern_df) + \n  geom_density(aes(x = rating_average), color = \"grey6\") + \n  theme_classic()\n\nggplot(data = pattern_df) +\n  geom_density(aes(x = rating_count),color = \"blue\") +\n  theme_classic()\n\npattern_df[is.na(pattern_df$rating_count),'rating_count'] &lt;- 0\n\n## Many patterns with a perfect 5\npattern_df[order(pattern_df$rating_average),]%&gt;% select(c(name,rating_average,rating_count)) %&gt;% tail(n = 10)\n\n## most rated patterns are around 4.5\npattern_df[order(pattern_df$rating_count),]%&gt;% select(c(name,rating_average,rating_count)) %&gt;% tail(n = 10)\n\n\nWarning message:\n\"Removed 32112 rows containing non-finite values (`stat_density()`).\"\n\n\n\n\n\n\nA data.frame: 10 × 3\n\n\n\nname\nrating_average\nrating_count\n\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n96235\n188-37 Opaline\n5\n3\n\n\n96246\nBanna Shawl\n5\n4\n\n\n96255\nMajestic Shawl\n5\n22\n\n\n96272\nJean Marie\n5\n4\n\n\n96274\n186-19 Aretusa\n5\n3\n\n\n96283\nForest Nymph\n5\n19\n\n\n96288\nSuper Simple Bulky Brioche Crossover\n5\n9\n\n\n96302\nWastelandic\n5\n5\n\n\n96307\nAlnwick Shawl\n5\n6\n\n\n96309\nHeadless Roses\n5\n10\n\n\n\n\n\n\nA data.frame: 10 × 3\n\n\n\nname\nrating_average\nrating_count\n\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1466\nGAP-tastic Cowl\n4.550026\n5807\n\n\n301\nBarley\n4.744859\n6079\n\n\n75\nFlax\n4.758879\n6138\n\n\n4255\nCalorimetry\n4.229793\n6619\n\n\n449\nHoney Cowl\n4.554268\n7592\n\n\n6658\nFetching\n4.335002\n7791\n\n\n1451\nMonkey Socks\n4.563994\n8032\n\n\n5649\nClapotis\n4.434231\n8507\n\n\n152\nHermione's Everyday Socks\n4.658873\n9067\n\n\n606\nHitchhiker\n4.678016\n9516\n\n\n\n\n\n\n\n\nIn a way that makes sense, the rating average is a value with high points at zero where patterns haven’t been rated or at a peak of roughly 4.5 before falling a bit to 5. This checks out as most ratings on the site are good or people wouldn’t bother, but there are also many unscored patterns. The count reflects this too with most patterns having zero but a long tail for some popular patterns which are listed as well.\n\n\nGauge, Gauge divisor, and Row gauge\nGauge is a value used to estimate dimensions based on the number of stitches that make up a 4x4 square. I’ll go into more detail after showing the distribution of the data.\n\n\nCode\ncheck &lt;- pattern_df$gauge %&gt;% table() %&gt;% as.data.frame() \ntop_10 &lt;- check[order(check$Freq),] %&gt;% tail(n = 10)\ntop_10\nlength(na.omit(pattern_df$gauge))\n\ncheck &lt;- pattern_df$gauge_divisor %&gt;% table() %&gt;% as.data.frame() \ntop_10 &lt;- check[order(check$Freq),] %&gt;% tail(n = 10)\ntop_10\npattern_df[is.na(pattern_df$rating_count),'gauge_divisor'] &lt;- 4\n\ncheck &lt;- pattern_df$row_gauge %&gt;% table() %&gt;% as.data.frame() \ntop_10 &lt;- check[order(check$Freq),] %&gt;% tail(n = 10)\ntop_10\nlength(na.omit(pattern_df$row_gauge))\n\n\n\nA data.frame: 10 × 2\n\n\n\n.\nFreq\n\n\n\n&lt;fct&gt;\n&lt;int&gt;\n\n\n\n\n184\n28\n2614\n\n\n175\n26\n2747\n\n\n123\n17\n2823\n\n\n145\n21\n3355\n\n\n195\n32\n3387\n\n\n116\n16\n4305\n\n\n128\n18\n5649\n\n\n163\n24\n5694\n\n\n152\n22\n6860\n\n\n139\n20\n7641\n\n\n\n\n\n80412\n\n\n\nA data.frame: 3 × 2\n\n\n\n.\nFreq\n\n\n\n&lt;fct&gt;\n&lt;int&gt;\n\n\n\n\n2\n2\n3407\n\n\n1\n1\n6131\n\n\n3\n4\n83978\n\n\n\n\n\n\nA data.frame: 10 × 2\n\n\n\n.\nFreq\n\n\n\n&lt;fct&gt;\n&lt;int&gt;\n\n\n\n\n109\n22\n2122\n\n\n101\n20\n2134\n\n\n163\n40\n2592\n\n\n148\n34\n2690\n\n\n120\n26\n3166\n\n\n154\n36\n3173\n\n\n115\n24\n4142\n\n\n139\n30\n4559\n\n\n144\n32\n5069\n\n\n128\n28\n5908\n\n\n\n\n\n70484\n\n\nGauge: this represents the number of stitches it takes to get a certain amount of inches, this amount of inches is usually 4 but is represented in the gauge divisor column of the data. There are a lot of missing values but as this is one of the main determining variables for the pattern I don’t think subbing in values will be feasible. I’ll just leave the empty values as is.\nGauge divisor: there were quite a few missing values but it was easier to replace these as the vast majority of the divisors were 4, so I just input 4 for the missing values. The value itself represents how many inches the gauge’s amount of stitches covers.\nRow Gauge: there are far fewer row gauge values listed which may be a limiting factor. Row gauge is similarly important to gauge itself, which tends to cover stitches rather than rows, but it has yet to be seen whether the combination of the two is a more powerful predictor than the thing itself. Similarly I don’t think these values can be filled in so I will leave them empty for now.\n\n\nYardage and Yardage_max\nOkay, so there is going to be a lot of work done on this value. Some people went absolutely crazy on the amount of yarn possible for a pattern with the difference between the max yardage and the yardage being at least a hundred thousand, there will be a lot of trimming here.\nWhen looking up values you get patterns like the captain america pillow with yardage listed from 1 to 1,000,000 which is utterly facetious. I’m going to do a few steps to reduce this. My first step will be culling patterns with wild numbers of yardage and max yardage. My follow up will be to set patterns with a NA amount of max yardage to the yardage of the rest of the pattern or vice versa. It is common to see patterns where it requires a set amount of yarn and there is no pattern sizing with maximum or minimum yardage.\n\n\nCode\n## trial and error to find cut off point\npattern_df %&gt;% filter(is.na(yardage_max)== FALSE) %&gt;% nrow()\npattern_df %&gt;% filter(yardage_max &lt; 5000) %&gt;% nrow()\n## I settled at around 5000 as it only removed 400 or so patterns\n## and its high enough that true high bounds won't get caught\n\n# implementing amount\npattern_df$yardage_max[pattern_df$yardage_max &gt; 5000] &lt;- NA \n\n## now to do the same for yardage, then we'll equate the two\n\npattern_df %&gt;% filter(is.na(yardage)== FALSE) %&gt;% nrow()\npattern_df %&gt;% filter(yardage &lt; 4000) %&gt;% nrow()\n\n## again settling around 4000 due to it taking only 200 or so patterns off\n## and being low enough to remove crazy value, I will replace these values with NA just to be sure\n\npattern_df$yardage[pattern_df$yardage &gt; 4000] &lt;- NA \n\n## now to set yardage_max to yardage when yardage_max is NA\n\n## I do regrettably believe this is a for loop moment\n\npattern_df2 &lt;- pattern_df\n\nfor(i in 1:nrow(pattern_df2)){\n  if(is.na(pattern_df2[i,21])==TRUE){\n    pattern_df2[i,21] &lt;- pattern_df2[i,20]\n  }\n}\n\n## now these two values should be equal\npattern_df2 %&gt;% filter(is.na(yardage_max)== FALSE) %&gt;% nrow()\npattern_df2 %&gt;% filter(is.na(yardage)== FALSE) %&gt;% nrow()\n\n## it seems that the yardage_max is actually a bit bigger so I'm going to do the same thing in reverse to even them out\n\nfor(i in 1:nrow(pattern_df2)){\n  if(is.na(pattern_df2[i,20])==TRUE){\n    pattern_df2[i,20] &lt;- pattern_df2[i,21]\n  }\n}\n\npattern_df2 %&gt;% filter(is.na(yardage_max)== FALSE) %&gt;% nrow()\npattern_df2 %&gt;% filter(is.na(yardage)== FALSE) %&gt;% nrow()\n\n# Now we are even\n\npattern_df &lt;- pattern_df2\n\n\nSo in total I have decreased the rows by about 500 to get rid of really wacky outliers and set values equal to their opposite when there were missing values. There are still around 10,000 patterns in the database without yardage but this is an affordable loss.\n\n\nYarn_weight_description\nYarn weight description was already formalized in the data cleaning tab.\n\n\nCode\nggplot(data = pattern_df, aes(x = yarn_weight_description,fill = \"red\")) + geom_bar() + theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nSo it is interesting that weight 4 or worsted/aran weight is the most popular. This isn’t the largest size but its popular cause you can make nice looking stitching without too much effort. It normally takes size 8-10 needles (american).\n\n\nNeedle Sizes Metric and US\n\n\nCode\nggplot(data = pattern_df) + \n  geom_density(aes(x = needle_sizes_metric), color = \"grey6\") + \n  geom_density(aes(x = needle_sizes_us),color = \"blue\") +\n  theme_classic()\n\npattern_df$needle_sizes_us %&gt;% na.omit() %&gt;% length()\npattern_df$needle_size_metric %&gt;% na.omit() %&gt;% length()\n\n\nWarning message:\n\"Removed 12844 rows containing non-finite values (`stat_density()`).\"\n\n\n83465\n\n\n0\n\n\n\n\n\nIn terms of missing values, there are more missing values for the US sizes than the metric. This is essentially a unit difference though so we can simply get around it by only using the metric values for analysis.\nAs expected, metric needle sizes, which only go to 25, peak a lot sooner than the US ones which go to 50. The averages do overlap a little with a large tail going to the right up to the max value. A density plot doesn’t fully capture the discrete nature of the data as sizes are fixed, but I do believe it depicts the distribution better than a bar chart would."
  },
  {
    "objectID": "eda/eda.html#updating-saved-data",
    "href": "eda/eda.html#updating-saved-data",
    "title": "Exploratory Data Analysis",
    "section": "Updating Saved Data",
    "text": "Updating Saved Data\nHaving trimmed and worked on the outliers, I will be resaving the data for future use:\n\n\nCode\nwrite.csv(pattern_df,\"../data/pattern_data_updated.csv\")"
  },
  {
    "objectID": "eda/eda.html#connections-and-hypothesis",
    "href": "eda/eda.html#connections-and-hypothesis",
    "title": "Exploratory Data Analysis",
    "section": "Connections and Hypothesis",
    "text": "Connections and Hypothesis\nI will assert that there is a relationship between the yardage, needle size, and gauge which reflects onto the pattern type and that price and yardage are also linked. There may also be a relationship there with difficulty as well and I additionally hope to visualize the text data some. I’m interested to see if this and other connections hold true on visualization:\n\nViz 1\n\n\nCode\nggplot(pattern_df,aes(y = yardage,x = needle_sizes_us,color = pattern_type,shape = craft), alpha = .3) + geom_jitter() +\n  theme_classic() + ylim(0,5000) +xlim(0,20)\n\n\nWarning message:\n\"Removed 23881 rows containing missing values (`geom_point()`).\"\n\n\n\n\n\nThere does seem to be some rudimentary distribution with middle sized needles using more yarn, but it’s overall clouded by the density of the data present. It would be possible to graph a sample and check distribution there or clump together types of patterns into samples representative of the projects, although that would be more based on a future clustering analysis or my own knowledge of patterns. However it is clear that the yardage lowers at higher needle sizes, as expected, and most types of pattern are well represented at all needle sizes.\n\n\nViz 2\nI have been wondering whether there is any relation of price to complexity or of the amount of yarn needed, assuming that a more complicated pattern to create would reflect more work and thus more compensation. Let’s see.\n\n\nCode\nggplot(pattern_df,aes(x = difficulty_average,y = yardage,color = free,alpha = .1)) + geom_jitter() + theme_classic() + ylim(0,5000) + xlim(1,7.5)\n\n\nWarning message:\n\"Removed 41119 rows containing missing values (`geom_point()`).\"\n\n\n\n\n\nAlright, there is a clear relationship that the more difficult a pattern is the more likely it is to be paid for, this could be a self fulfilling prophecy though as if someone pays for a pattern they will be more willing to rate its difficulty. I’ll do a follow up visualization without the difficulty and just free and yardage:\n\n\nCode\nggplot(data = pattern_df,aes(x = free,y = yardage,fill = free)) + geom_boxplot() +\n  theme_classic() + ylim(0,1000)\n\n\nWarning message:\n\"Removed 26367 rows containing non-finite values (`stat_boxplot()`).\"\n\n\n\n\n\nAlright, there was a ton more outlier noise than even I expected so I added limits on y to 1000 yards. But yeah looking at this I can see that my hypothesis was correct, free patterns usually don’t use as many yards of yarn as paid ones. Like this is actually super cool, I’ve been wondering about this for years.\n\n\nViz 3\nAs another visualization I’d want to take a look at the distribution of words used to describe patterns and also the words used in the names of the patterns themselves. I will do these both in python as there is more word cloud support in that language.\n\n\nCode\nimport pandas as pd\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\ntext_data = pd.read_csv(\"../data/patterns_data_text.csv\")\nname_data = pd.read_csv(\"../data/pattern_data_cleaned.csv\")\n\ntext_data = text_data[\"notes\"]\nname_data = name_data[\"name\"]\n\nprint(text_data.head())\n\ndef word_cloud_gen(my_text):\n    def plot_cloud(word_cloud):\n        plt.figure(figsize = (40, 30))\n        plt.imshow(wordcloud)\n        plt.axis(\"off\")\n\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000,\n        random_state = 1,\n        background_color = 'salmon',\n        colormap = 'Pastel1',\n        collocations = False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\n\n0    &gt; The Trifle Wrap is the first of 6\\r\\r\\n&gt; mys...\n1    The Skulls and Cones Halloween Crochet Sweater...\n2    **Use the code BOREALIS15 for 15% off the patt...\n3    **Use \"cozyfall23\" at check-out and enjoy a sw...\n4    Alex Scarf is a simple ribbed scarf that can b...\nName: notes, dtype: object\n\n\nC:\\Users\\duckd\\AppData\\Local\\Temp\\ipykernel_9268\\2079817723.py:6: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n  name_data = pd.read_csv(\"../data/pattern_data_cleaned.csv\")\n\n\n\n\nCode\ntext_data = text_data.astype('string').dropna()\ntext_list = text_data.tolist()\n\nword_cloud_gen(\" \".join(text_list))\nword_cloud_gen(\" \".join(name_data))\n\n\n\n\n\n\n\n\n\nI do like these two pictures of the words used for descriptions as I think they well represent the types of patterns and the facts about them, giving a top down view of the vocabulary of a knitter. The text itself is fairly expected with the descriptions having more technical words and shorthand while the names become representations of the types of patterns. It is interesting that hat and sock are the most common words while not being the most common pattern types in the data. This may be because other patterns with more general types do not need to include the name of what it is in the title. Very interesting.\nI think I’m going to bring one of these two to be on my introduction slide as they work well to create the sense of what sorts of information we are working with."
  },
  {
    "objectID": "eda/eda.html#conclusions",
    "href": "eda/eda.html#conclusions",
    "title": "Exploratory Data Analysis",
    "section": "Conclusions",
    "text": "Conclusions\nThis is possibly my favorite data set I’ve ever used and there’s alot to uncover here. Some relationships seem to be clear and as expected but others could show contradictory proof. There don’t seem to be too many strong linear relationships so more advanced methods may very much be needed in order to figure out the underlying patterns in the data. There is some space for improvement around normalizing data for analysis but much of the data is not normally distributed. Outliers make up a lot of the data but some patterns are far more popular than others and the story of those outliers themselves seem very interesting. Either way, I now have a strong foundation of knowledge on the data and I look forward to finding more specifics through advanced models."
  },
  {
    "objectID": "introduction/introduction.html",
    "href": "introduction/introduction.html",
    "title": "DSAN-5000: Introduction",
    "section": "",
    "text": "For the scope of my project, I will delve into the extensive dataset provided by Ravelry.com, which stands as the foremost interactive database for knitting and crochet patterns provided by users across the globe. This database, boasting an extensive collection of hundreds of thousands of patterns. Positioned as the primary resource for pattern discovery and utilization among knitting enthusiasts, there exists a compelling incentive to ensure the sustained engagement of users with the platform to help it maintain its supremacy. I would like to propose the application of machine learning and algorithmic models to help fill this void and provide better understanding of the data present to help with future targeting.\nDespite the significance of Ravelry.com in the knitting community, there is a noticeable absence of direct research aimed at algorithmically enhancing comprehension and targeting within this specific domain and knitting overall. Most other knitting databases are owned by private corporations that create patterns to sell themselves, Ravelry is the only public one and thus the public must help it. Nevertheless, a separate parallel can be drawn between my intended research focus and two distinct but related areas: fabric simulations and user targeting. The latter is a well-established field dedicated to tailoring content to users’ interests, thereby retaining their presence on the platform and driving targeted product engagement. I hope to incorporate those tactics into my work like private organizations would to build my own understanding of the methodology and to act as a consultant for Ravelry. On the other hand, the former represents a subfield within fabric simulations that specifically concentrates on knitted fabrics which, as a field, has been growing due to increased computational capacity. A more holistic understanding of the interplay between various patterns, yarn features, and fabric structures holds the potential to significantly enhance outcomes in fabric simulations and in pattern recommendations.\nI hope to serve as a conduit bridging these two spheres of knowledge together, conducting an initial probe that navigates the intersection of fabric simulations and user targeting by understanding the source of the two, patterns themselves. By doing so, I aim to fill the existing void in algorithmic enhancements for the knitting and crochet community on Ravelry.com. I do hope to also provide interesting facts and insights along the way which can help create a story with the data that can be pitched along with the analysis itself. I look forward to this work, and I also wish to provide some sources about fabric simulations as an example of what I’m mentioning."
  },
  {
    "objectID": "introduction/introduction.html#summary-of-project",
    "href": "introduction/introduction.html#summary-of-project",
    "title": "DSAN-5000: Introduction",
    "section": "",
    "text": "For the scope of my project, I will delve into the extensive dataset provided by Ravelry.com, which stands as the foremost interactive database for knitting and crochet patterns provided by users across the globe. This database, boasting an extensive collection of hundreds of thousands of patterns. Positioned as the primary resource for pattern discovery and utilization among knitting enthusiasts, there exists a compelling incentive to ensure the sustained engagement of users with the platform to help it maintain its supremacy. I would like to propose the application of machine learning and algorithmic models to help fill this void and provide better understanding of the data present to help with future targeting.\nDespite the significance of Ravelry.com in the knitting community, there is a noticeable absence of direct research aimed at algorithmically enhancing comprehension and targeting within this specific domain and knitting overall. Most other knitting databases are owned by private corporations that create patterns to sell themselves, Ravelry is the only public one and thus the public must help it. Nevertheless, a separate parallel can be drawn between my intended research focus and two distinct but related areas: fabric simulations and user targeting. The latter is a well-established field dedicated to tailoring content to users’ interests, thereby retaining their presence on the platform and driving targeted product engagement. I hope to incorporate those tactics into my work like private organizations would to build my own understanding of the methodology and to act as a consultant for Ravelry. On the other hand, the former represents a subfield within fabric simulations that specifically concentrates on knitted fabrics which, as a field, has been growing due to increased computational capacity. A more holistic understanding of the interplay between various patterns, yarn features, and fabric structures holds the potential to significantly enhance outcomes in fabric simulations and in pattern recommendations.\nI hope to serve as a conduit bridging these two spheres of knowledge together, conducting an initial probe that navigates the intersection of fabric simulations and user targeting by understanding the source of the two, patterns themselves. By doing so, I aim to fill the existing void in algorithmic enhancements for the knitting and crochet community on Ravelry.com. I do hope to also provide interesting facts and insights along the way which can help create a story with the data that can be pitched along with the analysis itself. I look forward to this work, and I also wish to provide some sources about fabric simulations as an example of what I’m mentioning."
  },
  {
    "objectID": "introduction/introduction.html#relevant-sources",
    "href": "introduction/introduction.html#relevant-sources",
    "title": "DSAN-5000: Introduction",
    "section": "Relevant Sources",
    "text": "Relevant Sources\n\nMechanics-aware deformation of yarn pattern geometry\n[see @sperl2021mechanics] This research paper undertook a noteworthy challenge by seeking to replace the prevailing industry standard, the triangle mesh fabric simulation method, with an innovative approach centered around knitted fabric simulation. The researchers achieved a commendable milestone, successfully implementing this novel method, albeit at a restricted scale. This strategic augmentation aimed to enhance the fidelity of fabric physics reproduction, pushing the boundaries to the extent of capturing intricate details such as yarn loop stretching.\nIn the pursuit of this groundbreaking endeavor, the researchers navigated the complexities of transitioning from the established triangle mesh paradigm to a more tailored knitted fabric simulation. This transition, while achieved at a limited size, lays the groundwork for potential advancements in fabric simulation techniques.\nThe successful integration of yarn layer animation over the traditional mesh not only demonstrates the adaptability of the proposed method but also underscores its potential applicability in broader contexts. The ability to capture yarn loop stretching is particularly noteworthy, as it signifies a breakthrough which could update the current paradigm and simplify the process for knitted fabrics in the future, saving computational power. This nuanced simulation approach may pave the way for more comprehensive studies into the behavior and optimization of knitted fabrics under diverse conditions, providing valuable insights for industries ranging from fashion to technical textiles.\n\n\nSimulating Knitted Cloth at the Yarn Level\n[see @kaldor2008simulating] In this pioneering research endeavor, the focus was squarely set on revolutionizing the conventional woven fabric model employed in fabric simulation by introducing a paradigm shift of adopting the interlocking loops characteristic of knitted fabrics. This is separate from the previous paper as it focuses less on recreating behavior but on simulating the yarn itself as one continuous thread at tension. This transformative departure represents a fundamental reorientation in the approach to fabric simulation, shifting the layer of analysis from the collective movement of an entire sheet of fabric to the intricate behavior of individual strands of yarn as mentioned.\nThe crux of this methodological shift lies in the nuanced simulation of yarn parameters, a departure from the more generalized approach inherent in traditional woven fabric simulations. By delving into the intricate characteristics of yarn such as tensile strength, weave, and fiber content, researchers sought to capture the complexities that arise from the interlocking loops present in knitted fabrics. This granular analysis enables a more detailed and faithful representation of the dynamic interplay between yarn strands, uncovering subtleties in movement, tension, and deformation that are integral to the authentic behavior of knitted fabrics.\nThe essence of this approach lies not only in the simulation of individual yarn strands but also in the subsequent extrapolation of these micro-level behaviors to construct larger scale models. Singular loops may be easy to model for instance, but simulating thousands all interacting with each other is a much more computationally dense task which this paper achieved. The ability to reproduce intricate yarn behaviors over broader fabric surfaces holds immense promise in advancing the understanding of knitted fabrics’ dynamic response to external forces, contributing to a more nuanced comprehension of their mechanical properties.\nAs this research ushers in a new era of fabric simulation methodologies, the implications extend beyond the immediate confines of academia. Industries reliant on accurate fabric representation, such as fashion, textiles, and materials science, stand to benefit significantly from these advancements in simulating fabrics before producing them or by modeling cuts of fabric before they are produced.. The potential for this novel approach to inform design processes, enhance manufacturing efficiency, and foster innovation across various sectors underscores the far-reaching impact of this paradigm change in fabric simulation."
  },
  {
    "objectID": "introduction/introduction.html#questions-for-research",
    "href": "introduction/introduction.html#questions-for-research",
    "title": "DSAN-5000: Introduction",
    "section": "Questions for Research",
    "text": "Questions for Research\n\nWhat relationships exist between known parameters?\nAre there groupings outside of known parameters that better represent the data?\nAre there any parameters that overlap and could reduce redundancy?\nWhat sort of keywords are most often used in pattern descriptions?\nWhat ways are there to measure popularity of a specific pattern?\nHow could a more tailored recommendation algorithm be built?\nAre some pattern types more popular than others?\nCould a machine learning process determine the kind of garment by the parameters?\nHow could pattern data be best correlated with real world trends if at all?\nWhat parameters could be used to create a more succinct pattern browsing experience?"
  }
]